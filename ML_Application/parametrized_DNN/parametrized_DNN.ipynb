{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30409ab8",
   "metadata": {},
   "source": [
    "# Parametrized DNN\n",
    "parametrized DNN \n",
    "1. Include all signals\n",
    "2. check the backgrounds.\n",
    "3. Check the AUC score as this is coming 1\n",
    "4. Improve the training\n",
    "5. include the weight of preselection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef20110a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X300_Y200/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X300_Y300/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X300_Y400/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X300_Y500/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X400_Y300/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X400_Y400/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X400_Y500/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X500_Y400/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X500_Y500/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X550_Y500/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X600_Y500/nominal/NOTAG_merged.parquet does not exist.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import uproot\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Taking mass X and corresponding Y mass points\n",
    "# mass_points = [300, 400, 500, 550, 600, 650, 700, 900, 1000, 1200, 1400, 1600, 1800, 2000, 2500, 3000, 3500, 4000]  # Example mass points\n",
    "# y_values = [ 60, 70, 80, 90, 95, 100, 125, 150, 200, 300, 400, 500, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000, 2600, 3000, 3500]  # Example Y values\n",
    "\n",
    "mass_points = [300, 400, 500, 550, 600, 650, 700, 900, 1000]  # Example mass points\n",
    "y_values = [ 100, 125, 150, 200, 300, 400, 500]  # Example Y values\n",
    "\n",
    "\n",
    "\n",
    "# Load signal data from Parquet files\n",
    "signal_data = []\n",
    "for mass in mass_points:\n",
    "    for y in y_values:\n",
    "        file_path = f\"../../../output_parquet/final_production_Syst/merged/NMSSM_X{mass}_Y{y}/nominal/NOTAG_merged.parquet\"\n",
    "        \n",
    "        if os.path.exists(file_path):  # Check if file exists\n",
    "            try:\n",
    "                df = pd.read_parquet(file_path)  # Load the Parquet file\n",
    "                df[\"mass\"] = mass  \n",
    "                df[\"y_value\"] = y  # Store Y value if needed\n",
    "                df[\"label\"] = 1  # Assuming signal label\n",
    "                signal_data.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not read {file_path}. Error: {e}\")\n",
    "        else:\n",
    "            print(f\"Warning: File {file_path} does not exist.\")\n",
    "\n",
    "# Combine all signal data into a single DataFrame\n",
    "signal_df = pd.concat(signal_data, ignore_index=True) if signal_data else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17e5fe94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(346281, 853)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71afef2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load background data from ROOT files\n",
    "background_files = [\n",
    "    (\"../../outputfiles/hhbbgg_analyzer-v2-trees.root\", \"/GGJets/preselection\"),\n",
    "    (\"../../outputfiles/hhbbgg_analyzer-v2-trees.root\", \"/GJetPt20To40/preselection\"),\n",
    "    (\"../../outputfiles/hhbbgg_analyzer-v2-trees.root\", \"/GJetPt40/preselection\"),\n",
    "#     (\"../../outputfiles/hhbbgg_analyzer-v2-trees.root\", \"/GGJets/preselection\"),\n",
    "#     (\"../../outputfiles/hhbbgg_analyzer-v2-trees.root\", \"/GJetPt20To40/preselection\"),\n",
    "#     (\"../../outputfiles/hhbbgg_analyzer-v2-trees.root\", \"/GJetPt40/preselection\"),\n",
    "]\n",
    "background_data = []\n",
    "for file_path, tree_name in background_files:\n",
    "    try:\n",
    "        with uproot.open(file_path) as file:\n",
    "            tree = file[tree_name]\n",
    "            df = tree.arrays(library=\"pd\")\n",
    "            df[\"mass\"] = np.random.choice(mass_points, len(df))  # Random mass assignment\n",
    "            df[\"label\"] = 0\n",
    "            background_data.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not read {file_path}. Error: {e}\")\n",
    "\n",
    "df_background = pd.concat(background_data, ignore_index=True) if background_data else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf8a3621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and labels\n",
    "features = [\n",
    "    'bbgg_eta', 'bbgg_phi', 'lead_pho_phi', 'sublead_pho_eta', \n",
    "    'sublead_pho_phi', 'diphoton_eta', 'diphoton_phi', 'dibjet_eta', 'dibjet_phi', \n",
    "    'lead_bjet_pt', 'sublead_bjet_pt', 'lead_bjet_eta', 'lead_bjet_phi', 'sublead_bjet_eta', \n",
    "    'sublead_bjet_phi', 'sublead_bjet_PNetB', 'lead_bjet_PNetB', 'CosThetaStar_gg', \n",
    "    'CosThetaStar_jj', 'CosThetaStar_CS', 'DeltaR_jg_min', 'pholead_PtOverM', \n",
    "    'phosublead_PtOverM', 'lead_pho_mvaID', 'sublead_pho_mvaID'\n",
    "]\n",
    "features.extend([\"mass\", \"y_value\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ad971e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random mass + y_value assignment for backgrounds (ensure this was done earlier!)\n",
    "df_background[\"mass\"] = np.random.choice(mass_points, len(df_background))\n",
    "df_background[\"y_value\"] = np.random.choice(y_values, len(df_background))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38a285f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class balance after dropping NaNs: 0    1199138\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Reduce background dataset size by random sampling\n",
    "background_fraction = 0.6 #  20% of the background\n",
    "df_background = df_background.sample(frac=background_fraction, random_state=42)\n",
    "\n",
    "# Combine signal and background\n",
    "df_combined = pd.concat([signal_df, df_background], ignore_index=True)\n",
    "df_combined = df_combined.dropna(subset=['weight_preselection'])  # ✅ drop rows with bad weights\n",
    "print(\"Class balance after dropping NaNs:\", df_combined['label'].value_counts())\n",
    "\n",
    "\n",
    "# checking df_combined is not empty\n",
    "if df_combined.empty:\n",
    "    raise ValueError(\"Error: Combined DataFrame is empty. Check input files.\")\n",
    "\n",
    "# Convert feature data to DataFrame to prevent AttributeError\n",
    "df_features = df_combined[features]\n",
    "\n",
    "# Fill missing values with column mean\n",
    "df_features = df_features.fillna(df_features.mean())\n",
    "\n",
    "# Extract features (X) and labels (y)\n",
    "X = df_features.values\n",
    "y = df_combined[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a158103a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signal shape (346281, 853) background.shape (1998563, 100)\n"
     ]
    }
   ],
   "source": [
    "# class balance\n",
    "print(\"signal shape\", signal_df.shape, \"background.shape\", df_background.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f6b05e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = ['weight_preselection']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f20ee71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into 80% train, 20% test (stratified to maintain label distribution)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4822fb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# Start fresh from combined_df\n",
    "X_with_meta = df_combined[features + ['label', 'weight_preselection']].copy()\n",
    "\n",
    "# Split using stratification\n",
    "train_df, test_df = train_test_split(\n",
    "    X_with_meta, test_size=0.2, random_state=42, stratify=X_with_meta['label']\n",
    ")\n",
    "\n",
    "# Extract arrays\n",
    "X_train = train_df[features].values\n",
    "y_train = train_df['label'].values\n",
    "# w_train = train_df['weight_preselection'].values\n",
    "\n",
    "X_test = test_df[features].values\n",
    "y_test = test_df['label'].values\n",
    "# w_test = test_df['weight_preselection'].values\n",
    "\n",
    "def normalize_weights(weights, labels):\n",
    "    sig_mask = labels == 1\n",
    "    bkg_mask = labels == 0\n",
    "    weights = np.array(weights, dtype=np.float32)\n",
    "\n",
    "    norm_weights = np.zeros_like(weights)\n",
    "    norm_weights[sig_mask] = 0.5 * weights[sig_mask] / np.sum(weights[sig_mask])\n",
    "    norm_weights[bkg_mask] = 0.5 * weights[bkg_mask] / np.sum(weights[bkg_mask])\n",
    "    return norm_weights\n",
    "\n",
    "# Start with raw preselections\n",
    "raw_w_train = train_df['weight_preselection'].values\n",
    "raw_w_test = test_df['weight_preselection'].values\n",
    "\n",
    "# Normalize to 0.5 sum per class\n",
    "w_train = normalize_weights(raw_w_train, y_train)\n",
    "w_test = normalize_weights(raw_w_test, y_test)\n",
    "assert not np.isnan(w_train).any(), \"Still NaNs in train weights!\"\n",
    "\n",
    "\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "train_weights_tensor = torch.tensor(w_train, dtype=torch.float32)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "test_weights_tensor = torch.tensor(w_test, dtype=torch.float32)\n",
    "\n",
    "# Create PyTorch datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor, train_weights_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor, test_weights_tensor)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4da9e3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define the Parametric DNN\n",
    "class ParameterizedDNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ParameterizedDNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(16, 1)  # Logits\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a091bdb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParameterizedDNN(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=27, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.3, inplace=False)\n",
       "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.3, inplace=False)\n",
       "    (8): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (9): ReLU()\n",
       "    (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Dropout(p=0.3, inplace=False)\n",
       "    (12): Linear(in_features=16, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize model\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "model = ParameterizedDNN(input_dim)\n",
    "criterion = nn.BCEWithLogitsLoss(reduction='none')  # Enable per-sample loss\n",
    "optimizer = Adam(model.parameters(), lr=0.001, weight_decay=1e-3)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3987f2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "def weighted_bce_loss(logits, targets, weights):\n",
    "    # logits: raw model outputs\n",
    "    # targets: binary labels (0 or 1)\n",
    "    # weights: per-sample weights (e.g. preselection * normalization)\n",
    "    loss = torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "        logits, targets, weight=weights, reduction='none'\n",
    "    )\n",
    "    return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8d407f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Only one class present in y_true. ROC AUC score is not defined in that case.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/sraj/ipykernel_1120253/1807660626.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0my_pred_train_binary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mtrain_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_train_binary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mtrain_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# Validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_binarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         return _average_binary_score(\n\u001b[0m\u001b[1;32m    573\u001b[0m             \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_binary_roc_auc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_fpr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_fpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/sklearn/metrics/_base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_binary_roc_auc_score\u001b[0;34m(y_true, y_score, sample_weight, max_fpr)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0;34m\"\"\"Binary roc auc score.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    340\u001b[0m             \u001b[0;34m\"Only one class present in y_true. ROC AUC score \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[0;34m\"is not defined in that case.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Only one class present in y_true. ROC AUC score is not defined in that case."
     ]
    }
   ],
   "source": [
    "# Training settings\n",
    "num_epochs = 50\n",
    "patience = 5\n",
    "best_auc = 0.0\n",
    "patience_counter = 0\n",
    "save_path = \"best_parametric_model.pt\"\n",
    "\n",
    "train_losses, train_accuracies, train_aucs = [], [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    y_true_train, y_pred_train = [], []\n",
    "\n",
    "    for X_batch, y_batch, w_batch in train_dataloader:\n",
    "        X_batch, y_batch, w_batch = X_batch.to(device), y_batch.to(device), w_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch).squeeze()\n",
    "        outputs = torch.clamp(outputs, min=-50.0, max=50.0)  # Prevent sigmoid overflow\n",
    "\n",
    "#         loss = criterion(outputs, y_batch)\n",
    "#         weighted_loss = (loss * w_batch).mean()\n",
    "        weighted_loss = weighted_bce_loss(outputs, y_batch, w_batch)\n",
    "        weighted_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += weighted_loss.item()\n",
    "\n",
    "        probs = torch.sigmoid(outputs).detach().cpu().numpy()\n",
    "        probs = np.nan_to_num(probs, nan=0.0, posinf=1.0, neginf=0.0)\n",
    "        y_pred_train.extend(probs)\n",
    "        y_true_train.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    # Metrics\n",
    "    y_pred_train_binary = (np.array(y_pred_train) > 0.5).astype(int)\n",
    "    train_accuracy = accuracy_score(y_true_train, y_pred_train_binary)\n",
    "    train_auc = roc_auc_score(y_true_train, y_pred_train)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs_test = model(X_test_tensor.to(device)).squeeze()\n",
    "        outputs_test = torch.clamp(outputs_test, min=-50.0, max=50.0)\n",
    "        probs_test = torch.sigmoid(outputs_test).cpu().numpy()\n",
    "        probs_test = np.nan_to_num(probs_test, nan=0.0, posinf=1.0, neginf=0.0)\n",
    "\n",
    "#         loss_test = criterion(outputs_test, y_test_tensor.to(device))\n",
    "#         test_loss = (loss_test * test_weights_tensor.to(device)).mean().item()\n",
    "        test_loss = weighted_bce_loss(outputs_test, y_test_tensor.to(device), test_weights_tensor.to(device)).item()\n",
    "\n",
    "\n",
    "        y_pred_test_binary = (probs_test > 0.5).astype(int)\n",
    "        test_accuracy = accuracy_score(y_test, y_pred_test_binary)\n",
    "        test_auc = roc_auc_score(y_test, probs_test)\n",
    "\n",
    "    train_losses.append(epoch_loss / len(train_dataloader))\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    train_aucs.append(train_auc)\n",
    "\n",
    "    print(f\"\\nEpoch [{epoch+1}/{num_epochs}]\")\n",
    "    print(f\"Train Loss: {train_losses[-1]:.4f} | Train Acc: {train_accuracy:.4f} | Train AUC: {train_auc:.4f}\")\n",
    "    print(f\"Test  Loss: {test_loss:.4f} | Test  Acc: {test_accuracy:.4f} | Test  AUC: {test_auc:.4f}\")\n",
    "\n",
    "    # Early Stopping\n",
    "    if test_auc > best_auc:\n",
    "        best_auc = test_auc\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f\"✅ Model improved. Saved to: {save_path}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement in AUC for {patience_counter} epoch(s).\")\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(\"⛔ Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "print(\"✅ Best model loaded from checkpoint.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc71a50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any NaNs in weights? False False\n",
      "Any Infs in weights? False False\n",
      "Weight stats: -1.410449e-07 7.8918065e-06 3.1272472e-07\n",
      "Label distribution: [1598850]\n"
     ]
    }
   ],
   "source": [
    "print(\"Any NaNs in weights?\", np.isnan(w_train).any(), np.isnan(w_test).any())\n",
    "print(\"Any Infs in weights?\", np.isinf(w_train).any(), np.isinf(w_test).any())\n",
    "print(\"Weight stats:\", np.min(w_train), np.max(w_train), np.mean(w_train))\n",
    "print(\"Label distribution:\", np.bincount(y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e53a1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_batch shape: torch.Size([64, 27])\n",
      "y_batch min/max: 0.0 0.0\n",
      "w_batch min/max: 7.05224465491483e-08 7.891806490079034e-06\n",
      "X_batch NaNs: False\n",
      "First 5 rows of X: tensor([[ 0.0697,  1.5136,  1.4884,  0.4126,  0.0128,  0.0348,  1.3392,  0.0653,\n",
      "         -0.5124, -0.1782, -0.1780,  0.0174, -0.2371,  0.1167, -1.4339, -0.3776,\n",
      "         -0.2196, -0.5763, -0.0898, -0.0410, -1.1316,  0.3156, -0.6895,  0.4607,\n",
      "          0.5037,  1.7881,  0.3299],\n",
      "        [ 0.7992, -1.3368,  1.4368, -0.0648, -0.2400,  0.1234,  1.3410,  1.1913,\n",
      "         -1.1651, -0.5242, -0.5191,  1.9290, -1.2705, -0.7345, -0.9372, -0.3659,\n",
      "         -0.3811,  0.2713,  1.5017, -1.2542,  0.2397, -0.2299,  0.1994,  0.5972,\n",
      "          0.5811,  0.3680, -1.0941],\n",
      "        [-0.6312,  0.8134,  0.1802, -0.7195, -1.5005, -0.3054,  0.1500, -0.1920,\n",
      "          1.6694, -0.0154,  1.0595, -0.0382, -1.1588, -0.1171,  0.9380, -0.1586,\n",
      "          3.8482,  0.6326,  0.0817, -0.3181, -0.4692,  0.2216, -0.9589,  0.6778,\n",
      "         -1.3542, -0.5788, -1.0941],\n",
      "        [-1.0158,  0.4182, -0.0611, -1.0876, -1.6160, -1.4503, -0.2372,  0.9631,\n",
      "          1.0005, -0.6654, -0.3171,  0.6176,  0.5955,  1.2806,  1.5059, -0.2253,\n",
      "         -0.3610, -1.0017, -0.6946, -1.2992,  2.7729, -0.0685, -0.3426,  0.5800,\n",
      "         -0.2145, -0.3421, -1.0941],\n",
      "        [-0.5444, -0.8241, -1.0318, -0.9668,  1.0233, -0.2969, -1.1886, -0.6352,\n",
      "          0.1211, -0.3570, -0.4541, -0.4946, -0.0889, -0.5075,  1.2401, -0.0872,\n",
      "         -0.3538,  0.9545, -0.1814,  0.4326, -1.0572,  0.0707, -1.2092,  0.6242,\n",
      "         -1.3901, -0.3421, -0.9161]])\n"
     ]
    }
   ],
   "source": [
    "for X_batch, y_batch, w_batch in train_dataloader:\n",
    "    print(\"X_batch shape:\", X_batch.shape)\n",
    "    print(\"y_batch min/max:\", y_batch.min().item(), y_batch.max().item())\n",
    "    print(\"w_batch min/max:\", w_batch.min().item(), w_batch.max().item())\n",
    "    print(\"X_batch NaNs:\", torch.isnan(X_batch).any().item())\n",
    "    print(\"First 5 rows of X:\", X_batch[:5])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d2a8b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
