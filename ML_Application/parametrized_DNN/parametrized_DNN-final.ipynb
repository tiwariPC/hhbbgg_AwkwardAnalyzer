{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30409ab8",
   "metadata": {},
   "source": [
    "# Parametrized DNN\n",
    "parametrized DNN \n",
    "1. Plot signal and background seperation\n",
    "2. Include all signals\n",
    "2. check the backgrounds.\n",
    "3. Check the AUC score as this is coming 1\n",
    "4. Improve the training\n",
    "5. include the weight of preselection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3c41b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X300_Y200/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X300_Y300/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X300_Y400/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X300_Y500/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X400_Y300/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X400_Y400/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X400_Y500/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X500_Y400/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X500_Y500/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X550_Y500/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X600_Y500/nominal/NOTAG_merged.parquet does not exist.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import uproot\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Taking mass X and corresponding Y mass points\n",
    "# mass_points = [300, 400, 500, 550, 600, 650, 700, 900, 1000, 1200, 1400, 1600, 1800, 2000, 2500, 3000, 3500, 4000]  # Example mass points\n",
    "# y_values = [ 60, 70, 80, 90, 95, 100, 125, 150, 200, 300, 400, 500, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000, 2600, 3000, 3500]  # Example Y values\n",
    "\n",
    "mass_points = [300, 400, 500, 550, 600, 650, 700, 900, 1000]  # Example mass points\n",
    "y_values = [ 100, 125, 150, 200, 300, 400, 500]  # Example Y values\n",
    "\n",
    "\n",
    "\n",
    "# Load signal data from Parquet files\n",
    "signal_data = []\n",
    "for mass in mass_points:\n",
    "    for y in y_values:\n",
    "        file_path = f\"../../../output_parquet/final_production_Syst/merged/NMSSM_X{mass}_Y{y}/nominal/NOTAG_merged.parquet\"\n",
    "        \n",
    "        if os.path.exists(file_path):  # Check if file exists\n",
    "            try:\n",
    "                df = pd.read_parquet(file_path)  # Load the Parquet file\n",
    "                df[\"mass\"] = mass  \n",
    "                df[\"y_value\"] = y  # Store Y value if needed\n",
    "                df[\"label\"] = 1  # Assuming signal label\n",
    "                signal_data.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not read {file_path}. Error: {e}\")\n",
    "        else:\n",
    "            print(f\"Warning: File {file_path} does not exist.\")\n",
    "\n",
    "# Combine all signal data into a single DataFrame\n",
    "signal_df = pd.concat(signal_data, ignore_index=True) if signal_data else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ace5651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(346281, 853)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9c83e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load background data from ROOT files\n",
    "background_files = [\n",
    "    (\"../../outputfiles/hhbbgg_analyzer-v2-trees.root\", \"/GGJets/preselection\"),\n",
    "    (\"../../outputfiles/hhbbgg_analyzer-v2-trees.root\", \"/GJetPt20To40/preselection\"),\n",
    "    (\"../../outputfiles/hhbbgg_analyzer-v2-trees.root\", \"/GJetPt40/preselection\"),\n",
    "#     (\"../../outputfiles/hhbbgg_analyzer-v2-trees.root\", \"/GGJets/preselection\"),\n",
    "#     (\"../../outputfiles/hhbbgg_analyzer-v2-trees.root\", \"/GJetPt20To40/preselection\"),\n",
    "#     (\"../../outputfiles/hhbbgg_analyzer-v2-trees.root\", \"/GJetPt40/preselection\"),\n",
    "]\n",
    "background_data = []\n",
    "for file_path, tree_name in background_files:\n",
    "    try:\n",
    "        with uproot.open(file_path) as file:\n",
    "            tree = file[tree_name]\n",
    "            df = tree.arrays(library=\"pd\")\n",
    "            df[\"mass\"] = np.random.choice(mass_points, len(df))  # Random mass assignment\n",
    "            df[\"label\"] = 0\n",
    "            background_data.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not read {file_path}. Error: {e}\")\n",
    "\n",
    "df_background = pd.concat(background_data, ignore_index=True) if background_data else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5521617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and labels\n",
    "features = [\n",
    "    'bbgg_eta', 'bbgg_phi', 'lead_pho_phi', 'sublead_pho_eta', \n",
    "    'sublead_pho_phi', 'diphoton_eta', 'diphoton_phi', 'dibjet_eta', 'dibjet_phi', \n",
    "    'lead_bjet_pt', 'sublead_bjet_pt', 'lead_bjet_eta', 'lead_bjet_phi', 'sublead_bjet_eta', \n",
    "    'sublead_bjet_phi', 'sublead_bjet_PNetB', 'lead_bjet_PNetB', 'CosThetaStar_gg', \n",
    "    'CosThetaStar_jj', 'CosThetaStar_CS', 'DeltaR_jg_min', 'pholead_PtOverM', \n",
    "    'phosublead_PtOverM', 'lead_pho_mvaID', 'sublead_pho_mvaID'\n",
    "]\n",
    "features.extend([\"mass\", \"y_value\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90b6ed8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bbgg_eta',\n",
       " 'bbgg_phi',\n",
       " 'lead_pho_phi',\n",
       " 'sublead_pho_eta',\n",
       " 'sublead_pho_phi',\n",
       " 'diphoton_eta',\n",
       " 'diphoton_phi',\n",
       " 'dibjet_eta',\n",
       " 'dibjet_phi',\n",
       " 'lead_bjet_pt',\n",
       " 'sublead_bjet_pt',\n",
       " 'lead_bjet_eta',\n",
       " 'lead_bjet_phi',\n",
       " 'sublead_bjet_eta',\n",
       " 'sublead_bjet_phi',\n",
       " 'sublead_bjet_PNetB',\n",
       " 'lead_bjet_PNetB',\n",
       " 'CosThetaStar_gg',\n",
       " 'CosThetaStar_jj',\n",
       " 'CosThetaStar_CS',\n",
       " 'DeltaR_jg_min',\n",
       " 'pholead_PtOverM',\n",
       " 'phosublead_PtOverM',\n",
       " 'lead_pho_mvaID',\n",
       " 'sublead_pho_mvaID',\n",
       " 'mass',\n",
       " 'y_value']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91c139ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random mass + y_value assignment for backgrounds (ensure this was done earlier!)\n",
    "df_background[\"mass\"] = np.random.choice(mass_points, len(df_background))\n",
    "df_background[\"y_value\"] = np.random.choice(y_values, len(df_background))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "458d6ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reduce background dataset size by random sampling\n",
    "background_fraction = 0.5 #  20% of the background\n",
    "df_background = df_background.sample(frac=background_fraction, random_state=42)\n",
    "\n",
    "# Combine signal and background\n",
    "df_combined = pd.concat([signal_df, df_background], ignore_index=True)\n",
    "\n",
    "# Ensure df_combined is not empty\n",
    "if df_combined.empty:\n",
    "    raise ValueError(\"Error: Combined DataFrame is empty. Check input files.\")\n",
    "\n",
    "# Convert feature data to DataFrame to prevent AttributeError\n",
    "df_features = df_combined[features]\n",
    "\n",
    "# Fill missing values with column mean\n",
    "df_features = df_features.fillna(df_features.mean())\n",
    "\n",
    "# Extract features (X) and labels (y)\n",
    "X = df_features.values\n",
    "y = df_combined[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34ac841f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2011751, 27)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1feb7f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into 80% train, 20% test (stratified to maintain label distribution)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Standardize features (Fit only on train, transform both)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)  \n",
    "X_test = scaler.transform(X_test)  \n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create Dataloader for training\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Optional: Create test dataloader if you want batch evaluation\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "beed08c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 300  400  500  550  600  650  700  900 1000]\n",
      "[1000  400  550  300  600  700  900  500  650]\n"
     ]
    }
   ],
   "source": [
    "# Checking\n",
    "\n",
    "print(signal_df[\"mass\"].unique())\n",
    "print(df_background[\"mass\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f75d48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['run', 'lumi', 'event', 'puppiMET_pt', 'puppiMET_phi',\n",
      "       'puppiMET_phiJERDown', 'puppiMET_phiJERUp', 'puppiMET_phiJESDown',\n",
      "       'puppiMET_phiJESUp', 'puppiMET_phiUnclusteredDown',\n",
      "       'puppiMET_phiUnclusteredUp', 'puppiMET_ptJERDown', 'puppiMET_ptJERUp',\n",
      "       'puppiMET_ptJESDown', 'puppiMET_ptJESUp', 'puppiMET_ptUnclusteredDown',\n",
      "       'puppiMET_ptUnclusteredUp', 'puppiMET_sumEt', 'lead_pho_pt',\n",
      "       'lead_pho_eta', 'lead_pho_phi', 'sublead_pho_pt', 'sublead_pho_eta',\n",
      "       'sublead_pho_phi', 'lead_bjet_pt', 'lead_bjet_eta', 'lead_bjet_phi',\n",
      "       'sublead_bjet_pt', 'sublead_bjet_eta', 'sublead_bjet_phi',\n",
      "       'dibjet_mass', 'diphoton_mass', 'bbgg_mass', 'dibjet_pt', 'diphoton_pt',\n",
      "       'bbgg_pt', 'bbgg_eta', 'bbgg_phi', 'weight_central',\n",
      "       'weight_preselection', 'weight_selection', 'weight_srbbgg',\n",
      "       'weight_srbbggMET', 'weight_crbbantigg', 'weight_crantibbgg',\n",
      "       'weight_crantibbantigg', 'weight_sideband', 'weight_idmva_sideband',\n",
      "       'weight_idmva_presel', 'dibjet_eta', 'dibjet_phi', 'diphoton_eta',\n",
      "       'diphoton_phi', 'lead_bjet_PNetB', 'sublead_bjet_PNetB',\n",
      "       'pholead_PtOverM', 'phosublead_PtOverM', 'FirstJet_PtOverM',\n",
      "       'SecondJet_PtOverM', 'CosThetaStar_CS', 'CosThetaStar_jj',\n",
      "       'CosThetaStar_gg', 'DeltaR_jg_min', 'lead_pt_over_diphoton_mass',\n",
      "       'sublead_pt_over_diphoton_mass', 'lead_pt_over_dibjet_mass',\n",
      "       'sublead_pt_over_dibjet_mass', 'diphoton_bbgg_mass', 'dibjet_bbgg_mass',\n",
      "       'lead_pho_mvaID_WP90', 'lead_pho_mvaID_WP80', 'sublead_pho_mvaID_WP90',\n",
      "       'sublead_pho_mvaID_WP80', 'lead_pho_mvaID', 'sublead_pho_mvaID',\n",
      "       'preselection', 'selection', 'srbbgg', 'srbbggMET', 'crantibbgg',\n",
      "       'crbbantigg', 'crantibbantigg', 'sideband', 'idmva_sideband',\n",
      "       'idmva_presel', 'DeltaR_j1g1', 'DeltaR_j2g1', 'DeltaR_j1g2',\n",
      "       'DeltaR_j2g2', 'mass', 'label', 'y_value'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_background.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2039f8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signal y_value: [100 125 150 200 300 400 500]\n",
      "Background y_value: [150 125 300 200 500 400 100]\n"
     ]
    }
   ],
   "source": [
    "print(\"Signal y_value:\", signal_df[\"y_value\"].unique())\n",
    "print(\"Background y_value:\", df_background[\"y_value\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01c9b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]\n",
      "Train Loss: 0.0075 | Train Acc: 0.9987 | Train AUC: 1.0000\n",
      "Test Loss: 0.0002 | Test Acc: 1.0000 | Test AUC: 1.0000\n",
      "Epoch [2/10]\n",
      "Train Loss: 0.0031 | Train Acc: 0.9998 | Train AUC: 1.0000\n",
      "Test Loss: 0.0003 | Test Acc: 1.0000 | Test AUC: 1.0000\n",
      "Epoch [3/10]\n",
      "Train Loss: 0.0032 | Train Acc: 0.9998 | Train AUC: 1.0000\n",
      "Test Loss: 0.0003 | Test Acc: 1.0000 | Test AUC: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "class ParameterizedDNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ParameterizedDNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(input_dim, 32),\n",
    "#             nn.ReLU(),\n",
    "#             nn.BatchNorm1d(32),  # Helps stabilize training\n",
    "#             nn.Dropout(0.2),  # Reduce dropout\n",
    "\n",
    "#             nn.Linear(32, 16),\n",
    "#             nn.ReLU(),\n",
    "#             nn.BatchNorm1d(16),\n",
    "#             nn.Dropout(0.2),\n",
    "            \n",
    "#             nn.Linear(16, 8),\n",
    "#             nn.ReLU(),\n",
    "#             nn.BatchNorm1d(8),\n",
    "#             nn.Dropout(0.1),  # Lower dropout to retain information\n",
    "            \n",
    "#             nn.Linear(8, 1)  # Output layer (raw logits)\n",
    "#         )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "input_dim = X.shape[1]\n",
    "model = ParameterizedDNN(input_dim)\n",
    "criterion = nn.BCEWithLogitsLoss()  # Expecting raw logits\n",
    "# criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([weight]))\n",
    "optimizer = Adam(model.parameters(), lr=0.001, weight_decay=1e-3)  # Reduce learning rate\n",
    "\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "train_aucs = []\n",
    "fpr_all, tpr_all, thresholds_all = [], [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    y_true_train, y_pred_train = [], []\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        X_batch, y_batch = batch\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch).squeeze()\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        y_true_train.extend(y_batch.cpu().numpy())\n",
    "        y_pred_train.extend(torch.sigmoid(outputs).detach().cpu().numpy())\n",
    "\n",
    "    # Convert predictions to binary\n",
    "    y_pred_train_binary = [1 if p > 0.5 else 0 for p in y_pred_train]\n",
    "    train_accuracy = accuracy_score(y_true_train, y_pred_train_binary)\n",
    "    train_auc = roc_auc_score(y_true_train, y_pred_train)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs_test = model(X_test_tensor.to(device)).squeeze()\n",
    "        y_pred_test = torch.sigmoid(outputs_test).cpu().numpy()\n",
    "        test_loss = criterion(outputs_test, y_test_tensor.to(device)).item()\n",
    "        \n",
    "        y_pred_test_binary = [1 if p > 0.5 else 0 for p in y_pred_test]\n",
    "        test_accuracy = accuracy_score(y_test, y_pred_test_binary)\n",
    "        test_auc = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "    # Print Progress\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "    print(f\"Train Loss: {epoch_loss / len(train_dataloader):.4f} | Train Acc: {train_accuracy:.4f} | Train AUC: {train_auc:.4f}\")\n",
    "    print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_accuracy:.4f} | Test AUC: {test_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573b2107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c1c56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Debugging\n",
    "interpolation_mass = 375  # between 300 and 400\n",
    "interpolation_y = 175     # between 150 and 200\n",
    "\n",
    "df_interp = df_background.sample(n=1000, random_state=42).copy()\n",
    "df_interp[\"mass\"] = interpolation_mass\n",
    "df_interp[\"y_value\"] = interpolation_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b25827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Choose an unseen MX, MY\n",
    "interpolation_mass = 375\n",
    "interpolation_y = 175\n",
    "\n",
    "# 2. Sample and set new param values\n",
    "df_interp = df_background.sample(n=1000, random_state=42).copy()\n",
    "df_interp[\"mass\"] = interpolation_mass\n",
    "df_interp[\"y_value\"] = interpolation_y\n",
    "\n",
    "# 3. Use the same features and preprocessing\n",
    "X_interp = df_interp[features].fillna(df_features.mean()).values\n",
    "X_interp = scaler.transform(X_interp)  # same scaler as training\n",
    "X_interp_tensor = torch.tensor(X_interp, dtype=torch.float32).to(device)\n",
    "\n",
    "# 4. Get model output\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_interp_pred = torch.sigmoid(model(X_interp_tensor)).cpu().numpy()\n",
    "\n",
    "# 5. Plot output distribution\n",
    "plt.hist(y_interp_pred, bins=50, alpha=0.7, label=f\"Interpolated (MX={interpolation_mass}, MY={interpolation_y})\")\n",
    "plt.xlabel(\"DNN Score\")\n",
    "plt.title(\"pNN Output at Interpolated Mass Point\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e0255e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afdc171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90293c12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7847187e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90a5118",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses.append(epoch_loss / len(train_dataloader))\n",
    "train_accuracies.append(train_accuracy)\n",
    "train_aucs.append(train_auc)\n",
    "\n",
    "# Add test AUC for plotting too\n",
    "if epoch == num_epochs - 1:\n",
    "    plt.plot(train_aucs, label='Train AUC')\n",
    "    plt.axhline(test_auc, color='red', linestyle='--', label='Test AUC (final)')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"AUC\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Train vs. Test AUC\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b3c714",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_pred_test[y_test == 1], bins=50, alpha=0.5, label='Signal')\n",
    "plt.hist(y_pred_test[y_test == 0], bins=50, alpha=0.5, label='Background')\n",
    "plt.legend()\n",
    "plt.title(\"DNN Output Scores on Test Set\")\n",
    "plt.xlabel(\"Score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b271432",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e99a94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e33a57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c170e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Evaluate model outputs on the full dataset\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_train_scores = torch.sigmoid(model(X_train_tensor.to(device))).cpu().numpy().flatten()\n",
    "    y_pred_test_scores = torch.sigmoid(model(X_test_tensor.to(device))).cpu().numpy().flatten()\n",
    "\n",
    "# Plot histograms\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Training scores\n",
    "plt.hist(y_pred_train_scores[y_train == 1], bins=50, alpha=0.6, label='Signal (Train)', color='red', histtype='stepfilled', density=True)\n",
    "plt.hist(y_pred_train_scores[y_train == 0], bins=50, alpha=0.6, label='Background (Train)', color='blue', histtype='stepfilled', density=True)\n",
    "\n",
    "# Test scores: Compute histograms, then plot centers\n",
    "def plot_points(y_scores, label, color):\n",
    "    counts, bins = np.histogram(y_scores, bins=50, range=(0, 1), density=True)\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "    plt.plot(bin_centers, counts, f'{color}o', label=label)\n",
    "\n",
    "plot_points(y_pred_test_scores[y_test == 1], 'Signal (Test)', 'r')\n",
    "plot_points(y_pred_test_scores[y_test == 0], 'Background (Test)', 'k')\n",
    "\n",
    "# Aesthetics\n",
    "plt.axvline(0.5, color='gray', linestyle='--')\n",
    "plt.xlabel(\"Classifier Output\")\n",
    "plt.ylabel(\"Normalized Events\")\n",
    "plt.title(\"Classifier Output with PyTorch\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69af4057",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_combined['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660a1824",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4729ac82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(range(1, num_epochs+1), train_losses, marker='o', linestyle='-', color='blue')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss vs. Epochs\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f5444a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Accuracy\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(range(1, num_epochs+1), train_accuracies, marker='o', linestyle='-', color='green')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs. Epochs\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2283b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot AUC\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(range(1, num_epochs+1), train_aucs, marker='o', linestyle='-', color='red')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.title(\"AUC vs. Epochs\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79363e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot AUC scores over epochs\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs+1), train_aucs, label=\"AUC\", color='blue', marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUC')\n",
    "plt.title('AUC Score over Epochs')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e247d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Plot the final ROC curve\n",
    "# Select the ROC curve from the last epoch\n",
    "fpr_last = fpr_all[-1]\n",
    "tpr_last = tpr_all[-1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr_last, tpr_last, color='darkorange', lw=2, label=f'ROC curve (AUC = {train_aucs[-1]:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # Random classifier line\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title(f'Final ROC Curve (AUC = {train_aucs[-1]:.2f})')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a4b1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve (last epoch)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_all[-1], tpr_all[-1], label=f\"AUC = {test_aucs[-1]:.4f}\")\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Metric plots\n",
    "epochs = range(1, num_epochs+1)\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
    "plt.plot(epochs, test_losses, label=\"Test Loss\")\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(epochs, train_accuracies, label=\"Train Acc\")\n",
    "plt.plot(epochs, test_accuracies, label=\"Test Acc\")\n",
    "plt.title(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(epochs, train_aucs, label=\"Train AUC\")\n",
    "plt.plot(epochs, test_aucs, label=\"Test AUC\")\n",
    "plt.title(\"AUC\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8ffdef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
