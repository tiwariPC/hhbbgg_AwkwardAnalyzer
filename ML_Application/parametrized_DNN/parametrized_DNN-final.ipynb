{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30409ab8",
   "metadata": {},
   "source": [
    "# Parametrized DNN\n",
    "parametrized DNN \n",
    "1. Plot signal and background seperation\n",
    "2. Include all signals\n",
    "2. check the backgrounds.\n",
    "3. Check the AUC score as this is coming 1\n",
    "4. Improve the training\n",
    "5. include the weight of preselection\n",
    "6. Weights for signal and background to negate the class imbalance\n",
    "7. Plot the DNN score for each sample. \n",
    "8. Event categorization \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3c41b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X300_Y200/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X300_Y300/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X300_Y400/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X300_Y500/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X400_Y300/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X400_Y400/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X400_Y500/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X500_Y400/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X500_Y500/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X550_Y500/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X600_Y500/nominal/NOTAG_merged.parquet does not exist.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import uproot\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Taking mass X and corresponding Y mass points\n",
    "# mass_points = [300, 400, 500, 550, 600, 650, 700, 900, 1000, 1200, 1400, 1600, 1800, 2000, 2500, 3000, 3500, 4000]  # Example mass points\n",
    "# y_values = [ 60, 70, 80, 90, 95, 100, 125, 150, 200, 300, 400, 500, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000, 2600, 3000, 3500]  # Example Y values\n",
    "\n",
    "mass_points = [300, 400, 500, 550, 600, 650, 700, 900, 1000]  #  mass points\n",
    "y_values = [ 100, 125, 150, 200, 300, 400, 500]  #  Y values\n",
    "\n",
    "\n",
    "\n",
    "# Load signal data from Parquet files\n",
    "signal_data = []\n",
    "for mass in mass_points:\n",
    "    for y in y_values:\n",
    "        file_path = f\"../../../output_parquet/final_production_Syst/merged/NMSSM_X{mass}_Y{y}/nominal/NOTAG_merged.parquet\"\n",
    "        \n",
    "        if os.path.exists(file_path):  # Check if file exists\n",
    "            try:\n",
    "                df = pd.read_parquet(file_path)  # Load the Parquet file\n",
    "                df[\"mass\"] = mass  \n",
    "                df[\"y_value\"] = y  # Store Y value if needed\n",
    "                df[\"label\"] = 1  # Assuming signal label\n",
    "                signal_data.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not read {file_path}. Error: {e}\")\n",
    "        else:\n",
    "            print(f\"Warning: File {file_path} does not exist.\")\n",
    "\n",
    "# Combine all signal data into a single DataFrame\n",
    "signal_df = pd.concat(signal_data, ignore_index=True) if signal_data else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ace5651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(346281, 853)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9c83e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load background data from ROOT files\n",
    "background_files = [\n",
    "    (\"../../outputfiles/hhbbgg_analyzer-v2-trees.root\", \"/GGJets/preselection\"),\n",
    "    (\"../../outputfiles/hhbbgg_analyzer-v2-trees.root\", \"/GJetPt20To40/preselection\"),\n",
    "    (\"../../outputfiles/hhbbgg_analyzer-v2-trees.root\", \"/GJetPt40/preselection\"),\n",
    "#     (\"../../outputfiles/hhbbgg_analyzer-v2-trees.root\", \"/GGJets/preselection\"),\n",
    "#     (\"../../outputfiles/hhbbgg_analyzer-v2-trees.root\", \"/GJetPt20To40/preselection\"),\n",
    "#     (\"../../outputfiles/hhbbgg_analyzer-v2-trees.root\", \"/GJetPt40/preselection\"),\n",
    "]\n",
    "background_data = []\n",
    "for file_path, tree_name in background_files:\n",
    "    try:\n",
    "        with uproot.open(file_path) as file:\n",
    "            tree = file[tree_name]\n",
    "            df = tree.arrays(library=\"pd\")\n",
    "            df[\"mass\"] = np.random.choice(mass_points, len(df))  # Random mass assignment\n",
    "            df[\"label\"] = 0\n",
    "            background_data.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not read {file_path}. Error: {e}\")\n",
    "\n",
    "df_background = pd.concat(background_data, ignore_index=True) if background_data else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5521617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and labels\n",
    "features = [\n",
    "    # bbgg varaibles\n",
    "    'bbgg_eta', 'bbgg_phi',\n",
    "    # Photon variables\n",
    "    'lead_pho_phi', 'sublead_pho_eta', 'sublead_pho_phi',\n",
    "    # Diphoton variables\n",
    "    'diphoton_eta', 'diphoton_phi',\n",
    "    # dibjet variables\n",
    "    'dibjet_eta', 'dibjet_phi', \n",
    "    # bjet kinematics\n",
    "    'lead_bjet_pt', 'sublead_bjet_pt', 'lead_bjet_eta', 'lead_bjet_phi', 'sublead_bjet_eta', \n",
    "    'sublead_bjet_phi', 'sublead_bjet_PNetB', 'lead_bjet_PNetB', \n",
    "    # collion-sopper frame variables.\n",
    "    'CosThetaStar_gg', \n",
    "    'CosThetaStar_jj', 'CosThetaStar_CS', \n",
    "    # Delta (jg, min)\n",
    "    'DeltaR_jg_min',\n",
    "    # Ratios\n",
    "    'pholead_PtOverM', 'phosublead_PtOverM',\n",
    "    # Pho mvaID\n",
    "    'lead_pho_mvaID', 'sublead_pho_mvaID'\n",
    "]\n",
    "features.extend([\"mass\", \"y_value\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b6ed8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c139ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random mass + y_value assignment for backgrounds (ensure this was done earlier!)\n",
    "df_background[\"mass\"] = np.random.choice(mass_points, len(df_background))\n",
    "df_background[\"y_value\"] = np.random.choice(y_values, len(df_background))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458d6ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reduce background dataset size by random sampling\n",
    "background_fraction = 1 #  20% of the background\n",
    "df_background = df_background.sample(frac=background_fraction, random_state=42)\n",
    "\n",
    "# Combine signal and background\n",
    "df_combined = pd.concat([signal_df, df_background], ignore_index=True)\n",
    "\n",
    "# checking df_combined is not empty\n",
    "if df_combined.empty:\n",
    "    raise ValueError(\"Error: Combined DataFrame is empty. Check input files.\")\n",
    "\n",
    "# Convert feature data to DataFrame to prevent AttributeError\n",
    "df_features = df_combined[features]\n",
    "\n",
    "# Fill missing values with column mean\n",
    "df_features = df_features.fillna(df_features.mean())\n",
    "\n",
    "# Extract features (X) and labels (y)\n",
    "X = df_features.values\n",
    "y = df_combined[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed93e6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = ['weight_preselection']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ac841f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feb7f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into 80% train, 20% test (stratified to maintain label distribution)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "## Adding weights\n",
    "# Compute class-balanced weights (sum of signal weights = 0.5, background = 0.5)\n",
    "def compute_class_normalized_weights(y):\n",
    "    signal_mask = y == 1\n",
    "    background_mask = y == 0\n",
    "\n",
    "    n_signal = np.sum(signal_mask)\n",
    "    n_background = np.sum(background_mask)\n",
    "\n",
    "    weights = np.zeros_like(y, dtype=np.float32)\n",
    "    weights[signal_mask] = 0.5 / n_signal\n",
    "    weights[background_mask] = 0.5 / n_background\n",
    "\n",
    "    return weights\n",
    "\n",
    "# Calculate weights for training and testing sets\n",
    "w_train = compute_class_normalized_weights(y_train)\n",
    "w_test = compute_class_normalized_weights(y_test)\n",
    "\n",
    "\n",
    "# Standardize features (Fit only on train, transform both)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)  \n",
    "X_test = scaler.transform(X_test)  \n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create Dataloader for training\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Optional: Create test dataloader if you want batch evaluation\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a82fbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute weights\n",
    "train_weights = compute_class_normalized_weights(y_train)\n",
    "test_weights = compute_class_normalized_weights(y_test)\n",
    "\n",
    "# Convert to torch tensors\n",
    "train_weights_tensor = torch.tensor(train_weights, dtype=torch.float32)\n",
    "test_weights_tensor = torch.tensor(test_weights, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5ec4ef",
   "metadata": {},
   "source": [
    "# Checking -----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8d696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Start fresh from combined_df\n",
    "X_with_meta = df_combined[features + ['label', 'weight_preselection']].copy()\n",
    "\n",
    "# Split using stratification\n",
    "train_df, test_df = train_test_split(\n",
    "    X_with_meta, test_size=0.2, random_state=42, stratify=X_with_meta['label']\n",
    ")\n",
    "\n",
    "# Extract arrays\n",
    "X_train = train_df[features].values\n",
    "y_train = train_df['label'].values\n",
    "w_train = train_df['weight_preselection'].values\n",
    "\n",
    "X_test = test_df[features].values\n",
    "y_test = test_df['label'].values\n",
    "w_test = test_df['weight_preselection'].values\n",
    "\n",
    "# Normalize weights (signal sum = 0.5, background sum = 0.5)\n",
    "def normalize_weights(weights, labels):\n",
    "    sig_mask = labels == 1\n",
    "    bkg_mask = labels == 0\n",
    "    weights = np.array(weights, dtype=np.float32)\n",
    "\n",
    "    norm_weights = np.zeros_like(weights)\n",
    "    norm_weights[sig_mask] = 0.5 * weights[sig_mask] / np.sum(weights[sig_mask])\n",
    "    norm_weights[bkg_mask] = 0.5 * weights[bkg_mask] / np.sum(weights[bkg_mask])\n",
    "    return norm_weights\n",
    "\n",
    "train_weights = normalize_weights(w_train, y_train)\n",
    "test_weights = normalize_weights(w_test, y_test)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "train_weights_tensor = torch.tensor(train_weights, dtype=torch.float32)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "test_weights_tensor = torch.tensor(test_weights, dtype=torch.float32)\n",
    "\n",
    "# Create PyTorch datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor, train_weights_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor, test_weights_tensor)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96fa259",
   "metadata": {},
   "source": [
    "# -------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a408f339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking\n",
    "\n",
    "print(signal_df[\"mass\"].unique())\n",
    "print(df_background[\"mass\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c4ee6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_background.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258ea236",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Signal y_value:\", signal_df[\"y_value\"].unique())\n",
    "print(\"Background y_value:\", df_background[\"y_value\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01c9b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "class ParameterizedDNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ParameterizedDNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(input_dim, 32),\n",
    "#             nn.ReLU(),\n",
    "#             nn.BatchNorm1d(32),  # Helps stabilize training\n",
    "#             nn.Dropout(0.2),  # Reduce dropout\n",
    "\n",
    "#             nn.Linear(32, 16),\n",
    "#             nn.ReLU(),\n",
    "#             nn.BatchNorm1d(16),\n",
    "#             nn.Dropout(0.2),\n",
    "            \n",
    "#             nn.Linear(16, 8),\n",
    "#             nn.ReLU(),\n",
    "#             nn.BatchNorm1d(8),\n",
    "#             nn.Dropout(0.1),  # Lower dropout to retain information\n",
    "            \n",
    "#             nn.Linear(8, 1)  # Output layer (raw logits)\n",
    "#         )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df3e456",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Mass distribution\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(signal_df['mass'], bins=20, alpha=0.5, label='Signal')\n",
    "plt.hist(df_background['mass'], bins=20, alpha=0.5, label='Background')\n",
    "plt.legend()\n",
    "plt.title(\"Mass Distribution\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71b2830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class balance\n",
    "print(\"signal shape\", signal_df.shape, \"background.shape\", df_background.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882835a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor, train_weights_tensor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f209a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Initialize model\n",
    "input_dim = X.shape[1]\n",
    "model = ParameterizedDNN(input_dim)\n",
    "criterion = nn.BCEWithLogitsLoss()  # Expecting raw logits\n",
    "optimizer = Adam(model.parameters(), lr=0.001, weight_decay=1e-3)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 50\n",
    "patience = 5\n",
    "best_auc = 0.0\n",
    "patience_counter = 0\n",
    "save_path = \"best_parametric_model.pt\"\n",
    "\n",
    "train_losses, train_accuracies, train_aucs = [], [], []\n",
    "fpr_all, tpr_all, thresholds_all = [], [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    y_true_train, y_pred_train = [], []\n",
    "\n",
    "    for X_batch, y_batch, w_batch in train_dataloader:\n",
    "        X_batch, y_batch, w_batch = X_batch.to(device), y_batch.to(device), w_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch).squeeze()\n",
    "\n",
    "        # ðŸ” Clamp logits to prevent sigmoid overflow\n",
    "        outputs = torch.clamp(outputs, min=-50.0, max=50.0)\n",
    "\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        weighted_loss = (loss * w_batch).mean()\n",
    "        weighted_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += weighted_loss.item()\n",
    "\n",
    "        # âœ… Safe prediction (remove NaNs if needed)\n",
    "        probs = torch.sigmoid(outputs).detach().cpu().numpy()\n",
    "        probs = np.nan_to_num(probs, nan=0.0, posinf=1.0, neginf=0.0)\n",
    "        y_pred_train.extend(probs)\n",
    "        y_true_train.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    y_pred_train_binary = [1 if p > 0.5 else 0 for p in y_pred_train]\n",
    "    train_accuracy = accuracy_score(y_true_train, y_pred_train_binary)\n",
    "    y_pred_train = np.array(y_pred_train)\n",
    "    y_true_train = np.array(y_true_train)\n",
    "\n",
    "    # Safe AUC\n",
    "\n",
    "    train_auc = roc_auc_score(y_true_train, y_pred_train)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs_test = model(X_test_tensor.to(device)).squeeze()\n",
    "\n",
    "        # ðŸ” Clamp logits before sigmoid\n",
    "        outputs_test = torch.clamp(outputs_test, min=-50.0, max=50.0)\n",
    "\n",
    "        probs_test = torch.sigmoid(outputs_test).cpu().numpy()\n",
    "\n",
    "        # âœ… Clean predictions\n",
    "        probs_test = np.nan_to_num(probs_test, nan=0.0, posinf=1.0, neginf=0.0)\n",
    "\n",
    "        test_loss = (criterion(outputs_test, y_test_tensor.to(device)) * test_weights_tensor.to(device)).mean().item()\n",
    "\n",
    "        y_pred_test_binary = [1 if p > 0.5 else 0 for p in probs_test]\n",
    "        test_accuracy = accuracy_score(y_test, y_pred_test_binary)\n",
    "        test_auc = roc_auc_score(y_test, probs_test)\n",
    "\n",
    "    train_losses.append(epoch_loss / len(train_dataloader))\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    train_aucs.append(train_auc)\n",
    "\n",
    "    print(f\"\\nEpoch [{epoch+1}/{num_epochs}]\")\n",
    "    print(f\"Train Loss: {train_losses[-1]:.4f} | Train Acc: {train_accuracy:.4f} | Train AUC: {train_auc:.4f}\")\n",
    "    print(f\"Test  Loss: {test_loss:.4f} | Test  Acc: {test_accuracy:.4f} | Test  AUC: {test_auc:.4f}\")\n",
    "\n",
    "    # Early stopping based on test AUC\n",
    "    if test_auc > best_auc:\n",
    "        best_auc = test_auc\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f\"âœ… Model improved. Saved to: {save_path}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement in AUC for {patience_counter} epoch(s).\")\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(\"â›” Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "# Load best model before proceeding\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "print(\"âœ… Best model loaded from checkpoint.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8201ae17",
   "metadata": {},
   "source": [
    "# Next step\n",
    "\n",
    "- [ ] Implement early stopping\n",
    "- [ ] Save the model\n",
    "- [ ] get the dNN score\n",
    "- [ ] Plot it like as in the approval of manos\n",
    "- [ ] Can also plot the signal and background seperations\n",
    "- [ ] Plot AUC\n",
    "- [ ] AUC, ROC\n",
    "- [ ] Signal and background separation\n",
    "- [ ] Check all variables. Include all variables \t\n",
    "- [ ] pNN transformed score\n",
    "- [ ] Plot feature importance \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab81588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Debugging\n",
    "interpolation_mass = 375  # between 300 and 400\n",
    "interpolation_y = 175     # between 150 and 200\n",
    "\n",
    "df_interp = df_background.sample(n=1000, random_state=42).copy()\n",
    "df_interp[\"mass\"] = interpolation_mass\n",
    "df_interp[\"y_value\"] = interpolation_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505f99eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Choose an unseen MX, MY\n",
    "interpolation_mass = 375\n",
    "interpolation_y = 175\n",
    "\n",
    "# 2. Sample and set new param values\n",
    "df_interp = df_background.sample(n=1000, random_state=42).copy()\n",
    "df_interp[\"mass\"] = interpolation_mass\n",
    "df_interp[\"y_value\"] = interpolation_y\n",
    "\n",
    "# 3. Use the same features and preprocessing\n",
    "X_interp = df_interp[features].fillna(df_features.mean()).values\n",
    "X_interp = scaler.transform(X_interp)  # same scaler as training\n",
    "X_interp_tensor = torch.tensor(X_interp, dtype=torch.float32).to(device)\n",
    "\n",
    "# 4. Get model output\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_interp_pred = torch.sigmoid(model(X_interp_tensor)).cpu().numpy()\n",
    "\n",
    "# 5. Plot output distribution\n",
    "plt.hist(y_interp_pred, bins=50, alpha=0.7, label=f\"Interpolated (MX={interpolation_mass}, MY={interpolation_y})\")\n",
    "plt.xlabel(\"DNN Score\")\n",
    "plt.title(\"pNN Output at Interpolated Mass Point\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411e7bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance(Error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295ee408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def compute_permutation_importance(model, X_val, y_val, mass_val, feature_names, device='cpu'):\n",
    "    model.eval()\n",
    "    X_val = X_val.clone().detach().to(device)\n",
    "    mass_val = mass_val.clone().detach().to(device)\n",
    "    y_val_np = y_val.cpu().numpy()\n",
    "\n",
    "    base_preds = torch.sigmoid(model(X_val, mass_val).squeeze()).detach().cpu().numpy()\n",
    "    base_auc = roc_auc_score(y_val_np, base_preds)\n",
    "\n",
    "    importances = []\n",
    "\n",
    "    for i in range(X_val.shape[1]):\n",
    "        scores = []\n",
    "        for _ in range(5):  # repeat for stability\n",
    "            X_shuffled = X_val.clone()\n",
    "            idx = torch.randperm(X_val.shape[0])\n",
    "            X_shuffled[:, i] = X_shuffled[idx, i]\n",
    "            preds = torch.sigmoid(model(X_shuffled, mass_val).squeeze()).detach().cpu().numpy()\n",
    "            auc = roc_auc_score(y_val_np, preds)\n",
    "            scores.append(base_auc - auc)  # performance drop\n",
    "        importances.append(np.mean(scores))\n",
    "\n",
    "    return np.array(importances)\n",
    "\n",
    "# Call the function\n",
    "importances = compute_permutation_importance(model, X_test_tensor, y_test_tensor, mass_test_tensor, feature_names, device=device)\n",
    "\n",
    "# Plot\n",
    "sorted_idx = np.argsort(importances)[::-1]\n",
    "plt.figure(figsize=(10, 12))\n",
    "plt.barh(np.array(feature_names)[sorted_idx], importances[sorted_idx])\n",
    "plt.xlabel(\"Permutation Importance (AUC drop)\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\"Feature Importance (Permutation - Parametric DNN)\")\n",
    "plt.xscale('log')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5206b15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c18ec0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41bcc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses.append(epoch_loss / len(train_dataloader))\n",
    "train_accuracies.append(train_accuracy)\n",
    "train_aucs.append(train_auc)\n",
    "\n",
    "# Add test AUC for plotting too\n",
    "if epoch == num_epochs - 1:\n",
    "    plt.plot(train_aucs, label='Train AUC')\n",
    "    plt.axhline(test_auc, color='red', linestyle='--', label='Test AUC (final)')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"AUC\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Train vs. Test AUC\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842b409b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_pred_test[y_test == 1], bins=50, alpha=0.5, label='Signal')\n",
    "plt.hist(y_pred_test[y_test == 0], bins=50, alpha=0.5, label='Background')\n",
    "plt.legend()\n",
    "plt.title(\"DNN Output Scores on Test Set\")\n",
    "plt.xlabel(\"Score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88873a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bkg = df_background.copy()\n",
    "df_bkg[\"mass\"] = 500\n",
    "df_bkg[\"y_value\"] = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca78c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sig = signal_df[(signal_df['mass'] == 500) & (signal_df['y_value'] == 200)].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780ab405",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bkg = df_background.copy()\n",
    "df_bkg[\"mass\"] = 500\n",
    "df_bkg[\"y_value\"] = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b6c24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, scaler, features, fallback_mean=None):\n",
    "    df_features_local = df[features].copy()\n",
    "\n",
    "    # Fallback to global mean if passed (e.g., from training data)\n",
    "    if fallback_mean is not None:\n",
    "        df_features_local = df_features_local.fillna(fallback_mean)\n",
    "    else:\n",
    "        df_features_local = df_features_local.fillna(df_features_local.mean())\n",
    "\n",
    "    X = df_features_local.values\n",
    "    X_scaled = scaler.transform(X)\n",
    "    return torch.tensor(X_scaled, dtype=torch.float32).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84795bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a45f6c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69af4057",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_combined['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95960288",
   "metadata": {},
   "source": [
    "## Fixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8ffdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Combine signal and background into a single DataFrame\n",
    "full_df = pd.concat([signal_df, df_background], ignore_index=True)\n",
    "\n",
    "# Define train/test mass points\n",
    "train_masses = [300, 500, 600, 700, 1000]\n",
    "test_masses = [400, 550, 650, 900]\n",
    "\n",
    "# Split by mass\n",
    "train_df = full_df[full_df['mass'].isin(train_masses)]\n",
    "test_df = full_df[full_df['mass'].isin(test_masses)]\n",
    "\n",
    "# Shuffle rows\n",
    "train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "test_df = test_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Define your feature list (must match your actual feature engineering)\n",
    "features = [col for col in train_df.columns if col not in ['label']]  # include 'mass' and 'y_value' if needed\n",
    "\n",
    "# Prepare inputs and targets\n",
    "X_train = train_df[features].fillna(0).values\n",
    "y_train = train_df['label'].values\n",
    "X_test = test_df[features].fillna(0).values\n",
    "y_test = test_df['label'].values\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Wrap in datasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f999db0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7d47aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec66a67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
