{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3b59d91",
   "metadata": {},
   "source": [
    "# Parametric DNN – Clean Version (Balanced Training Only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbcf3d6",
   "metadata": {},
   "source": [
    "parametrized DNN \n",
    "1. Plot signal and background seperation\n",
    "2. Include all signals\n",
    "2. check the backgrounds.\n",
    "3. Check the AUC score as this is coming 1\n",
    "4. Improve the training\n",
    "5. include the weight of preselection\n",
    "6. Weights for signal and background to negate the class imbalance\n",
    "7. Plot the DNN score for each sample. \n",
    "8. Event categorization \n",
    "9. check all of the code and implementation of weights\n",
    "10. Check model\n",
    "    1. Check loss functions\n",
    "11. fix the error. \n",
    "12. Check the imbalance of data in the signal and backgrounds.\n",
    "\n",
    "\n",
    "July 22\n",
    "1. Fix the background and then go through the model and try to get a nice output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da41b7d",
   "metadata": {},
   "source": [
    "events -> apply weights -> include mass and assign randomly--> Class imbalance --> Fix the class imbalance -> training model --> DNN score --> Event categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "072720dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import uproot\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e2354e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X300_Y200/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X300_Y300/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X300_Y400/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X300_Y500/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X300_Y600/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X300_Y800/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X400_Y300/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X400_Y400/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X400_Y500/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X400_Y600/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X400_Y800/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X500_Y400/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X500_Y500/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X500_Y600/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X500_Y800/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X550_Y500/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X550_Y600/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X550_Y800/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X600_Y80/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X600_Y500/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X600_Y600/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X600_Y800/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X650_Y95/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X650_Y600/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X650_Y800/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X700_Y600/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X700_Y800/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X900_Y60/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X900_Y800/nominal/NOTAG_merged.parquet does not exist.\n"
     ]
    }
   ],
   "source": [
    "mass_points = [300, 400, 500, 550, 600, 650, 700, 900, 1000]  #  mass points\n",
    "y_values = [60, 70, 80, 90, 95, 100, 125, 150, 200, 300, 400, 500, 600, 800]  #  Y values\n",
    "\n",
    "\n",
    "\n",
    "# Load signal data from Parquet files\n",
    "signal_data = []\n",
    "for mass in mass_points:\n",
    "    for y in y_values:\n",
    "        file_path = f\"../../../output_parquet/final_production_Syst/merged/NMSSM_X{mass}_Y{y}/nominal/NOTAG_merged.parquet\"\n",
    "        \n",
    "        if os.path.exists(file_path):  # Check if file exists\n",
    "            try:\n",
    "                df = pd.read_parquet(file_path)  # Load the Parquet file\n",
    "                df[\"mass\"] = mass  \n",
    "                df[\"y_value\"] = y  # Store Y value if needed\n",
    "                df[\"label\"] = 1  # Assuming signal label\n",
    "                signal_data.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not read {file_path}. Error: {e}\")\n",
    "        else:\n",
    "            print(f\"Warning: File {file_path} does not exist.\")\n",
    "\n",
    "# Combine all signal data into a single DataFrame\n",
    "signal_df = pd.concat(signal_data, ignore_index=True) if signal_data else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8083c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(626552, 853)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e129de16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load background data from ROOT files\n",
    "background_files = [\n",
    "    (\"../../outputfiles/hhbbgg_analyzer-v2-trees.root\", \"/GGJets/preselection\"),\n",
    "    (\"../../outputfiles/hhbbgg_analyzer-v2-trees.root\", \"/GJetPt20To40/preselection\"),\n",
    "    (\"../../outputfiles/hhbbgg_analyzer-v2-trees.root\", \"/GJetPt40/preselection\"),\n",
    "#     (\"../../outputfiles/hhbbgg_analyzer-v2-trees.root\", \"/GGJets/preselection\"),\n",
    "#     (\"../../outputfiles/hhbbgg_analyzer-v2-trees.root\", \"/GJetPt20To40/preselection\"),\n",
    "#     (\"../../outputfiles/hhbbgg_analyzer-v2-trees.root\", \"/GJetPt40/preselection\"),\n",
    "]\n",
    "background_data = []\n",
    "for file_path, tree_name in background_files:\n",
    "    try:\n",
    "        with uproot.open(file_path) as file:\n",
    "            tree = file[tree_name]\n",
    "            df = tree.arrays(library=\"pd\")\n",
    "#             df[\"mass\"] = np.random.choice(mass_points, len(df))  # Random mass assignment\n",
    "            df[\"label\"] = 0\n",
    "            background_data.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not read {file_path}. Error: {e}\")\n",
    "\n",
    "df_background = pd.concat(background_data, ignore_index=True) if background_data else pd.DataFrame()\n",
    "# Assigning random mass to Y\n",
    "df_background[\"mass\"] = np.random.choice(mass_points, size=len(df_background))\n",
    "df_background[\"y_value\"] = np.random.choice(y_values, size=len(df_background))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faec2778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and labels\n",
    "features = [\n",
    "    # bbgg varaibles\n",
    "    'bbgg_eta', 'bbgg_phi',\n",
    "    # Photon variables\n",
    "    'lead_pho_phi', 'sublead_pho_eta', 'sublead_pho_phi',\n",
    "    # Diphoton variables\n",
    "    'diphoton_eta', 'diphoton_phi',\n",
    "    # dibjet variables\n",
    "    'dibjet_eta', 'dibjet_phi', \n",
    "    # bjet kinematics\n",
    "    'lead_bjet_pt', 'sublead_bjet_pt', 'lead_bjet_eta', 'lead_bjet_phi', 'sublead_bjet_eta', \n",
    "    'sublead_bjet_phi', 'sublead_bjet_PNetB', 'lead_bjet_PNetB', \n",
    "    # collion-sopper frame variables.\n",
    "    'CosThetaStar_gg', \n",
    "    'CosThetaStar_jj', 'CosThetaStar_CS', \n",
    "    # Delta (jg, min)\n",
    "    'DeltaR_jg_min',\n",
    "    # Ratios\n",
    "    'pholead_PtOverM', 'phosublead_PtOverM',\n",
    "    # Pho mvaID\n",
    "    'lead_pho_mvaID', 'sublead_pho_mvaID'\n",
    "]\n",
    "features.extend([\"mass\", \"y_value\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "165d3f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random mass + y_value assignment for backgrounds (ensure this was done earlier!)\n",
    "# df_background[\"mass\"] = np.random.choice(mass_points, len(df_background))\n",
    "# df_background[\"y_value\"] = np.random.choice(y_values, len(df_background))\n",
    "\n",
    "\n",
    "# Sample (mass, y_value) pairs from signal and assign to background\n",
    "# sampled_mass_y = signal_df[[\"mass\", \"y_value\"]].sample(n=len(df_background), replace=True).reset_index(drop=True)\n",
    "# df_background[\"mass\"] = sampled_mass_y[\"mass\"]\n",
    "# df_background[\"y_value\"] = sampled_mass_y[\"y_value\"]\n",
    "\n",
    "# Get signal distribution weights\n",
    "signal_mass_y = signal_df[[\"mass\", \"y_value\"]]\n",
    "value_counts = signal_mass_y.value_counts(normalize=True).reset_index()\n",
    "value_counts.columns = [\"mass\", \"y_value\", \"weight\"]\n",
    "\n",
    "# Sample using these weights\n",
    "sampled_mass_y = value_counts.sample(\n",
    "    n=len(df_background), \n",
    "    replace=True, \n",
    "    weights=\"weight\", \n",
    "    random_state=42\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Assign to background\n",
    "df_background[\"mass\"] = sampled_mass_y[\"mass\"]\n",
    "df_background[\"y_value\"] = sampled_mass_y[\"y_value\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c5e068",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reduce background dataset size by random sampling\n",
    "background_fraction = 0.6 #  20% of the background\n",
    "df_background = df_background.sample(frac=background_fraction, random_state=42)\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Upsample signal\n",
    "signal_upsampled = resample(signal_df, replace=True, n_samples=len(df_background), random_state=42)\n",
    "df_combined = pd.concat([signal_upsampled, df_background], ignore_index=True)\n",
    "\n",
    "\n",
    "# Combine signal and background\n",
    "# df_combined = pd.concat([signal_df, df_background], ignore_index=True)\n",
    "\n",
    "# checking df_combined is not empty\n",
    "if df_combined.empty:\n",
    "    raise ValueError(\"Error: Combined DataFrame is empty. Check input files.\")\n",
    "\n",
    "# Convert feature data to DataFrame to prevent AttributeError\n",
    "df_features = df_combined[features]\n",
    "\n",
    "# Fill missing values with column mean\n",
    "df_features = df_features.fillna(df_features.mean())\n",
    "\n",
    "# Extract features (X) and labels (y)\n",
    "X = df_features.values\n",
    "y = df_combined[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcc57de",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = ['weight_preselection']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4e6335",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into 80% train, 20% test (stratified to maintain label distribution)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "## Adding weights\n",
    "# Compute class-balanced weights (sum of signal weights = 0.5, background = 0.5)\n",
    "# def compute_class_normalized_weights(y):\n",
    "#     signal_mask = y == 1\n",
    "#     background_mask = y == 0\n",
    "\n",
    "#     n_signal = np.sum(signal_mask)\n",
    "#     n_background = np.sum(background_mask)\n",
    "\n",
    "#     weights = np.zeros_like(y, dtype=np.float32)\n",
    "#     weights[signal_mask] = 0.5 / n_signal\n",
    "#     weights[background_mask] = 0.5 / n_background\n",
    "\n",
    "#     return weights\n",
    "\n",
    "def compute_class_normalized_weights(y):\n",
    "    signal_mask = y == 1\n",
    "    background_mask = y == 0\n",
    "\n",
    "    n_signal = np.sum(signal_mask)\n",
    "    n_background = np.sum(background_mask)\n",
    "\n",
    "    if n_signal == 0 or n_background == 0:\n",
    "        raise ValueError(\"One of the classes is missing in the split.\")\n",
    "\n",
    "    weights = np.zeros_like(y, dtype=np.float32)\n",
    "    weights[signal_mask] = 1.0 / n_signal\n",
    "    weights[background_mask] = 1.0 / n_background\n",
    "    weights *= len(y)  # Rescale so total weight = total number of samples\n",
    "\n",
    "    return weights\n",
    "\n",
    "\n",
    "# Calculate weights for training and testing sets\n",
    "w_train = compute_class_normalized_weights(y_train)\n",
    "w_test = compute_class_normalized_weights(y_test)\n",
    "\n",
    "\n",
    "# Standardize features (Fit only on train, transform both)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)  \n",
    "X_test = scaler.transform(X_test)  \n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Create PyTorch datasets\n",
    "# train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create Dataloader for training\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Optional: Create test dataloader if you want batch evaluation\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7305d5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute weights\n",
    "train_weights = compute_class_normalized_weights(y_train)\n",
    "test_weights = compute_class_normalized_weights(y_test)\n",
    "\n",
    "# Convert to torch tensors\n",
    "train_weights_tensor = torch.tensor(train_weights, dtype=torch.float32)\n",
    "test_weights_tensor = torch.tensor(test_weights, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922fdb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "class ParameterizedDNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ParameterizedDNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.Dropout(0.3),\n",
    "        \n",
    "\n",
    "            nn.Linear(8, 1)\n",
    "        )\n",
    "\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(input_dim, 32),\n",
    "#             nn.ReLU(),\n",
    "#             nn.BatchNorm1d(32),  # Helps stabilize training\n",
    "#             nn.Dropout(0.2),  # Reduce dropout\n",
    "\n",
    "#             nn.Linear(32, 16),\n",
    "#             nn.ReLU(),\n",
    "#             nn.BatchNorm1d(16),\n",
    "#             nn.Dropout(0.2),\n",
    "            \n",
    "#             nn.Linear(16, 8),\n",
    "#             nn.ReLU(),\n",
    "#             nn.BatchNorm1d(8),\n",
    "#             nn.Dropout(0.1),  # Lower dropout to retain information\n",
    "            \n",
    "#             nn.Linear(8, 1)  # Output layer (raw logits)\n",
    "#         )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131222e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mass distribution\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(signal_df['mass'], bins=20, alpha=0.5, label='Signal')\n",
    "plt.hist(df_background['mass'], bins=20, alpha=0.5, label='Background')\n",
    "plt.legend()\n",
    "plt.title(\"Mass Distribution\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3476cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class ParameterizedDNN(nn.Module):\n",
    "#     def __init__(self, input_dim):\n",
    "#         super(ParameterizedDNN, self).__init__()\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(input_dim, 128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.BatchNorm1d(128),\n",
    "#             nn.Linear(128, 64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.BatchNorm1d(64),\n",
    "#             nn.Linear(64, 32),\n",
    "#             nn.ReLU(),\n",
    "#             nn.BatchNorm1d(32),\n",
    "#             nn.Linear(32, 1)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fea3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Example placeholders – REPLACE with actual data loading\n",
    "# # For demonstration, we simulate signal and background\n",
    "\n",
    "# np.random.seed(42)\n",
    "# n_sig, n_bkg = 30000, 3000000\n",
    "# n_features = 30\n",
    "\n",
    "# signal = np.random.normal(1, 1, size=(n_sig, n_features))\n",
    "# background = np.random.normal(0, 1, size=(n_bkg, n_features))\n",
    "\n",
    "# X = np.vstack([signal, background])\n",
    "# y = np.array([1]*n_sig + [0]*n_bkg)\n",
    "\n",
    "# features = [f\"f{i}\" for i in range(n_features)]  # Placeholder feature names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f0fae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Balance the dataset\n",
    "# df = pd.DataFrame(X, columns=features)\n",
    "# df[\"label\"] = y\n",
    "\n",
    "# signal_df = df[df[\"label\"] == 1]\n",
    "# background_df = df[df[\"label\"] == 0]\n",
    "\n",
    "# background_ds = resample(background_df, n_samples=len(signal_df), random_state=42)\n",
    "# df_balanced = pd.concat([signal_df, background_ds])\n",
    "\n",
    "# X_bal = df_balanced.drop(\"label\", axis=1).values\n",
    "# y_bal = df_balanced[\"label\"].values\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_bal, y_bal, test_size=0.2, stratify=y_bal, random_state=42)\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "# X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "# y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "# X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "# y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e47e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = ParameterizedDNN(X_train.shape[1]).to(device)\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "# optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# model.train()\n",
    "# for epoch in range(10):\n",
    "#     epoch_loss = 0\n",
    "#     y_pred_train, y_true_train = [], []\n",
    "\n",
    "#     for xb, yb in train_loader:\n",
    "#         xb, yb = xb.to(device), yb.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(xb).squeeze()\n",
    "#         loss = criterion(outputs, yb)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         probs = torch.sigmoid(outputs).detach().cpu().numpy()\n",
    "#         y_pred_train.extend(probs)\n",
    "#         y_true_train.extend(yb.cpu().numpy())\n",
    "#         epoch_loss += loss.item()\n",
    "\n",
    "#     auc = roc_auc_score(y_true_train, y_pred_train)\n",
    "#     acc = accuracy_score(y_true_train, [1 if p > 0.5 else 0 for p in y_pred_train])\n",
    "#     print(f\"Epoch {epoch+1} | Loss: {epoch_loss:.4f} | AUC: {auc:.4f} | Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62109f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     test_outputs = model(X_test_tensor.to(device)).squeeze()\n",
    "#     test_probs = torch.sigmoid(test_outputs).cpu().numpy()\n",
    "\n",
    "# plt.hist(test_probs[y_test == 1], bins=50, alpha=0.5, label=\"Signal\")\n",
    "# plt.hist(test_probs[y_test == 0], bins=50, alpha=0.5, label=\"Background\")\n",
    "# plt.xlabel(\"Model Output\")\n",
    "# plt.ylabel(\"Count\")\n",
    "# plt.legend()\n",
    "# plt.title(\"Model output distribution on balanced data\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e6395f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ========== Weights ==========\n",
    "def compute_class_normalized_weights(y):\n",
    "    signal_mask = y == 1\n",
    "    background_mask = y == 0\n",
    "\n",
    "    n_signal = np.sum(signal_mask)\n",
    "    n_background = np.sum(background_mask)\n",
    "\n",
    "    weights = np.zeros_like(y, dtype=np.float32)\n",
    "    weights[signal_mask] = 0.5/ n_signal\n",
    "    weights[background_mask] = 0.5 / n_background\n",
    "    weights *= len(y)\n",
    "\n",
    "    return weights\n",
    "\n",
    "w_train = compute_class_normalized_weights(y_train)\n",
    "\n",
    "# ========== Standardization ==========\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# ========== Torch Tensors ==========\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "w_train_tensor = torch.tensor(w_train, dtype=torch.float32)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# ========== Dataset & DataLoader ==========\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor, w_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# ========== Model, Loss, Optimizer ==========\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ParameterizedDNN(X_train.shape[1]).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(reduction='none')\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# ========== Training Loop ==========\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    y_pred_train, y_true_train = [], []\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for xb, yb, wb in train_loader:\n",
    "        xb, yb, wb = xb.to(device), yb.to(device), wb.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(xb).squeeze()\n",
    "        loss = criterion(outputs, yb)\n",
    "        weighted_loss = (loss * wb).mean()\n",
    "        weighted_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        probs = torch.sigmoid(outputs).detach().cpu().numpy()\n",
    "        y_pred_train.extend(probs)\n",
    "        y_true_train.extend(yb.cpu().numpy())\n",
    "        epoch_loss += weighted_loss.item()\n",
    "\n",
    "    auc = roc_auc_score(y_true_train, y_pred_train)\n",
    "    acc = accuracy_score(y_true_train, [1 if p > 0.5 else 0 for p in y_pred_train])\n",
    "    print(f\"Epoch {epoch+1} | Loss: {epoch_loss:.4f} | AUC: {auc:.4f} | Accuracy: {acc:.4f}\")\n",
    "\n",
    "# ========== Evaluation Plot ==========\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_probs = torch.sigmoid(model(X_test_tensor.to(device)).squeeze()).cpu().numpy()\n",
    "\n",
    "plt.hist(test_probs[y_test == 1], bins=50, alpha=0.5, label=\"Signal\")\n",
    "plt.hist(test_probs[y_test == 0], bins=50, alpha=0.5, label=\"Background\")\n",
    "plt.xlabel(\"Model Output\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Output Distribution on Imbalanced Dataset\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1d4f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if y_test has both signal and background\n",
    "print(\"Signal count in test:\", np.sum(y_test == 1))\n",
    "print(\"Background count in test:\", np.sum(y_test == 0))\n",
    "\n",
    "# Check model output distribution\n",
    "test_logits = model(X_test_tensor.to(device)).squeeze()\n",
    "test_probs = torch.sigmoid(test_logits).detach().cpu().numpy()\n",
    "\n",
    "print(\"\\nModel output stats:\")\n",
    "print(\"Min:\", test_probs.min())\n",
    "print(\"Max:\", test_probs.max())\n",
    "print(\"Mean:\", test_probs.mean())\n",
    "\n",
    "# Quick overall output histogram\n",
    "plt.hist(test_probs, bins=50)\n",
    "plt.title(\"Overall Model Output Distribution\")\n",
    "plt.xlabel(\"Output (after sigmoid)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9241fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(test_probs[y_test == 1], bins=50, alpha=0.5, label=\"Signal\")\n",
    "plt.hist(test_probs[y_test == 0], bins=50, alpha=0.5, label=\"Background\")\n",
    "plt.xlabel(\"Model Output\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.title(\"Signal vs Background Output Distribution\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b727fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "auc = roc_auc_score(y_test, test_probs)\n",
    "print(f\"AUC: {auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e450bbae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edce37d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68752353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3073752",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483d1476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=3)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Training Accuracy:\", clf.score(X_train, y_train))\n",
    "print(\"Testing Accuracy:\", clf.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680d2fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Correlation with label\n",
    "import pandas as pd\n",
    "\n",
    "df_features = pd.DataFrame(X_train, columns=features)\n",
    "df_features[\"label\"] = y_train\n",
    "\n",
    "correlations = df_features.corr()[\"label\"].abs().sort_values(ascending=False)\n",
    "print(correlations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a99d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_test, bins=2)\n",
    "plt.xticks([0, 1], ['Background', 'Signal'])\n",
    "plt.title(\"Test Set Class Distribution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af72f678",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31570b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccd5b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(test_probs[y_test == 1], bins=50, alpha=0.5, label=\"Signal\")\n",
    "plt.hist(test_probs[y_test == 0], bins=50, alpha=0.5, label=\"Background\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Model Output\")\n",
    "plt.ylabel(\"Count (log scale)\")\n",
    "plt.legend()\n",
    "plt.title(\"Model Output Distribution (log)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
