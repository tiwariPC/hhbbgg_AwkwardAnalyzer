{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37a257b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import uproot\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import Adam\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cf0732f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X300_Y200/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X300_Y300/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X300_Y400/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X300_Y500/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X400_Y300/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X400_Y400/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X400_Y500/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X500_Y400/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X500_Y500/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X550_Y500/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X600_Y500/nominal/NOTAG_merged.parquet does not exist.\n"
     ]
    }
   ],
   "source": [
    "# Taking mass X and corresponding Y mass points\n",
    "# mass_points = [300, 400, 500, 550, 600, 650, 700, 900, 1000, 1200, 1400, 1600, 1800, 2000, 2500, 3000, 3500, 4000]  # Example mass points\n",
    "# y_values = [ 60, 70, 80, 90, 95, 100, 125, 150, 200, 300, 400, 500, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000, 2600, 3000, 3500]  # Example Y values\n",
    "\n",
    "mass_points = [300, 400, 500, 550, 600, 650, 700, 900, 1000]  # Example mass points\n",
    "y_values = [ 100, 125, 150, 200, 300, 400, 500]  # Example Y values\n",
    "\n",
    "\n",
    "\n",
    "# Load signal data from Parquet files\n",
    "signal_data = []\n",
    "for mass in mass_points:\n",
    "    for y in y_values:\n",
    "        file_path = f\"../../../output_parquet/final_production_Syst/merged/NMSSM_X{mass}_Y{y}/nominal/NOTAG_merged.parquet\"\n",
    "        \n",
    "        if os.path.exists(file_path):  # Check if file exists\n",
    "            try:\n",
    "                df = pd.read_parquet(file_path)  # Load the Parquet file\n",
    "                df[\"mass\"] = mass  \n",
    "                df[\"y_value\"] = y  # Store Y value if needed\n",
    "                df[\"label\"] = 1  # Assuming signal label\n",
    "                signal_data.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not read {file_path}. Error: {e}\")\n",
    "        else:\n",
    "            print(f\"Warning: File {file_path} does not exist.\")\n",
    "\n",
    "# Combine all signal data into a single DataFrame\n",
    "signal_df = pd.concat(signal_data, ignore_index=True) if signal_data else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93dcc228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(346281, 853)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eddf8057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load background data from ROOT files\n",
    "background_files = [\n",
    "    (\"../../outputfiles/hhbbgg_analyzer-v2-trees.root\", \"/GGJets/preselection\"),\n",
    "    (\"../../outputfiles/hhbbgg_analyzer-v2-trees.root\", \"/GJetPt20To40/preselection\"),\n",
    "    (\"../../outputfiles/hhbbgg_analyzer-v2-trees.root\", \"/GJetPt40/preselection\"),\n",
    "    (\"../../outputfiles/hhbbgg_analyzer-v2-trees.root\", \"/ttHToGG/preselection\"),\n",
    "#     (\"../../outputfiles/hhbbgg_analyzer-v2-trees.root\", \"/GJetPt20To40/preselection\"),\n",
    "#     (\"../../outputfiles/hhbbgg_analyzer-v2-trees.root\", \"/GJetPt40/preselection\"),\n",
    "]\n",
    "background_data = []\n",
    "for file_path, tree_name in background_files:\n",
    "    try:\n",
    "        with uproot.open(file_path) as file:\n",
    "            tree = file[tree_name]\n",
    "            df = tree.arrays(library=\"pd\")\n",
    "#             df[\"mass\"] = np.random.choice(mass_points, len(df))  # Random mass assignment\n",
    "            df[\"label\"] = 0\n",
    "            background_data.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not read {file_path}. Error: {e}\")\n",
    "\n",
    "df_background = pd.concat(background_data, ignore_index=True) if background_data else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8253c158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and labels\n",
    "features = [\n",
    "    'bbgg_eta', 'bbgg_phi', 'lead_pho_phi', 'sublead_pho_eta', \n",
    "    'sublead_pho_phi', 'diphoton_eta', 'diphoton_phi', 'dibjet_eta', 'dibjet_phi', \n",
    "    'lead_bjet_pt', 'sublead_bjet_pt', 'lead_bjet_eta', 'lead_bjet_phi', 'sublead_bjet_eta', \n",
    "    'sublead_bjet_phi', 'sublead_bjet_PNetB', 'lead_bjet_PNetB', 'CosThetaStar_gg', \n",
    "    'CosThetaStar_jj', 'CosThetaStar_CS', 'DeltaR_jg_min', 'pholead_PtOverM', \n",
    "    'phosublead_PtOverM', 'lead_pho_mvaID', 'sublead_pho_mvaID'\n",
    "]\n",
    "# features.extend([\"mass\", \"y_value\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e5861f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random mass + y_value assignment for backgrounds (ensure this was done earlier!)\n",
    "df_background[\"mass\"] = np.random.choice(mass_points, len(df_background))\n",
    "df_background[\"y_value\"] = np.random.choice(y_values, len(df_background))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "75dee4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reduce background dataset size by random sampling\n",
    "background_fraction = 0.1 #  20% of the background\n",
    "df_background = df_background.sample(frac=background_fraction, random_state=42)\n",
    "\n",
    "# Combine signal and background\n",
    "df_combined = pd.concat([signal_df, df_background], ignore_index=True)\n",
    "\n",
    "# Ensure df_combined is not empty\n",
    "if df_combined.empty:\n",
    "    raise ValueError(\"Error: Combined DataFrame is empty. Check input files.\")\n",
    "\n",
    "# Convert feature data to DataFrame to prevent AttributeError\n",
    "df_features = df_combined[features]\n",
    "\n",
    "# Fill missing values with column mean\n",
    "df_features = df_features.fillna(df_features.mean())\n",
    "\n",
    "# Extract features (X) and labels (y)\n",
    "X = df_features.values\n",
    "y = df_combined[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4f42dfff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(359150, 84)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_background.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "144b932f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(705431, 25)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "09b4a870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into 80% train, 20% test (stratified to maintain label distribution)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Standardize features (Fit only on train, transform both)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)  \n",
    "X_test = scaler.transform(X_test)  \n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create Dataloader for training\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Optional: Create test dataloader if you want batch evaluation\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "83144ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "class ParameterizedDNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ParameterizedDNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),  # Helps stabilize training\n",
    "            nn.Dropout(0.2),  # Reduce dropout\n",
    "\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.1),  # Lower dropout to retain information\n",
    "            \n",
    "            nn.Linear(64, 1)  # Output layer (raw logits)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "99494ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 256]           6,656\n",
      "              ReLU-2                  [-1, 256]               0\n",
      "       BatchNorm1d-3                  [-1, 256]             512\n",
      "           Dropout-4                  [-1, 256]               0\n",
      "            Linear-5                  [-1, 256]          65,792\n",
      "              ReLU-6                  [-1, 256]               0\n",
      "       BatchNorm1d-7                  [-1, 256]             512\n",
      "           Dropout-8                  [-1, 256]               0\n",
      "            Linear-9                  [-1, 128]          32,896\n",
      "             ReLU-10                  [-1, 128]               0\n",
      "      BatchNorm1d-11                  [-1, 128]             256\n",
      "          Dropout-12                  [-1, 128]               0\n",
      "           Linear-13                   [-1, 64]           8,256\n",
      "             ReLU-14                   [-1, 64]               0\n",
      "      BatchNorm1d-15                   [-1, 64]             128\n",
      "          Dropout-16                   [-1, 64]               0\n",
      "           Linear-17                    [-1, 1]              65\n",
      "================================================================\n",
      "Total params: 115,073\n",
      "Trainable params: 115,073\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 0.44\n",
      "Estimated Total Size (MB): 0.46\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "input_dim = X.shape[1]\n",
    "model = ParameterizedDNN(input_dim).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()  # Expecting raw logits\n",
    "# criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([weight]))\n",
    "optimizer = Adam(model.parameters(), lr=0.001, weight_decay=1e-5)  # Reduce learning rate\n",
    "summary(model,input_size=(input_dim,) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1869cc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]\n",
      "Train Loss: 0.0006 | Train Acc: 0.9999 | Train AUC: 1.0000\n",
      "Test Loss: 0.0000 | Test Acc: 1.0000 | Test AUC: 1.0000\n",
      "Epoch [2/10]\n",
      "Train Loss: 0.0002 | Train Acc: 1.0000 | Train AUC: 1.0000\n",
      "Test Loss: 0.0000 | Test Acc: 1.0000 | Test AUC: 1.0000\n",
      "Epoch [3/10]\n",
      "Train Loss: 0.0001 | Train Acc: 1.0000 | Train AUC: 1.0000\n",
      "Test Loss: 0.0000 | Test Acc: 1.0000 | Test AUC: 1.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/sraj/ipykernel_2629978/115095739.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_grads_batched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "train_aucs = []\n",
    "fpr_all, tpr_all, thresholds_all = [], [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    y_true_train, y_pred_train = [], []\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        X_batch, y_batch = batch\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch).squeeze()\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        y_true_train.extend(y_batch.cpu().numpy())\n",
    "        y_pred_train.extend(torch.sigmoid(outputs).detach().cpu().numpy())\n",
    "\n",
    "    # Convert predictions to binary\n",
    "    y_pred_train_binary = [1 if p > 0.5 else 0 for p in y_pred_train]\n",
    "    train_accuracy = accuracy_score(y_true_train, y_pred_train_binary)\n",
    "    train_auc = roc_auc_score(y_true_train, y_pred_train)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs_test = model(X_test_tensor.to(device)).squeeze()\n",
    "        y_pred_test = torch.sigmoid(outputs_test).cpu().numpy()\n",
    "        test_loss = criterion(outputs_test, y_test_tensor.to(device)).item()\n",
    "        \n",
    "        y_pred_test_binary = [1 if p > 0.5 else 0 for p in y_pred_test]\n",
    "        test_accuracy = accuracy_score(y_test, y_pred_test_binary)\n",
    "        test_auc = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "    # Print Progress\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "    print(f\"Train Loss: {epoch_loss / len(train_dataloader):.4f} | Train Acc: {train_accuracy:.4f} | Train AUC: {train_auc:.4f}\")\n",
    "    print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_accuracy:.4f} | Test AUC: {test_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d1d7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Evaluate model outputs on the full dataset\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_train_scores = torch.sigmoid(model(X_train_tensor.to(device))).cpu().numpy().flatten()\n",
    "    y_pred_test_scores = torch.sigmoid(model(X_test_tensor.to(device))).cpu().numpy().flatten()\n",
    "\n",
    "# Plot histograms\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Training scores\n",
    "plt.hist(y_pred_train_scores[y_train == 1], bins=50, alpha=0.6, label='Signal (Train)', color='red', histtype='stepfilled', density=True)\n",
    "plt.hist(y_pred_train_scores[y_train == 0], bins=50, alpha=0.6, label='Background (Train)', color='blue', histtype='stepfilled', density=True)\n",
    "\n",
    "# Test scores: Compute histograms, then plot centers\n",
    "def plot_points(y_scores, label, color):\n",
    "    counts, bins = np.histogram(y_scores, bins=50, range=(0, 1), density=True)\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "    plt.plot(bin_centers, counts, f'{color}o', label=label)\n",
    "\n",
    "plot_points(y_pred_test_scores[y_test == 1], 'Signal (Test)', 'r')\n",
    "plot_points(y_pred_test_scores[y_test == 0], 'Background (Test)', 'k')\n",
    "\n",
    "# Aesthetics\n",
    "plt.axvline(0.5, color='gray', linestyle='--')\n",
    "plt.xlabel(\"Classifier Output\")\n",
    "plt.ylabel(\"Normalized Events\")\n",
    "plt.title(\"Classifier Output with PyTorch\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521073b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(range(1, num_epochs+1), train_losses, marker='o', linestyle='-', color='blue')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss vs. Epochs\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6936bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Accuracy\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(range(1, num_epochs+1), train_accuracies, marker='o', linestyle='-', color='green')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs. Epochs\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8104b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot AUC\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(range(1, num_epochs+1), train_aucs, marker='o', linestyle='-', color='red')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.title(\"AUC vs. Epochs\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f43482a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot AUC scores over epochs\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs+1), train_aucs, label=\"AUC\", color='blue', marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUC')\n",
    "plt.title('AUC Score over Epochs')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a4d8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Plot the final ROC curve\n",
    "# Select the ROC curve from the last epoch\n",
    "fpr_last = fpr_all[-1]\n",
    "tpr_last = tpr_all[-1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr_last, tpr_last, color='darkorange', lw=2, label=f'ROC curve (AUC = {train_aucs[-1]:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # Random classifier line\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title(f'Final ROC Curve (AUC = {train_aucs[-1]:.2f})')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaf2379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80aebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_check = pd.DataFrame(X_train, columns=features)\n",
    "df_check['label'] = y_train\n",
    "print(df_check.corr()['label'].sort_values(ascending=False).head(10))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
