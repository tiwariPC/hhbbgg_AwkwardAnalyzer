{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ce6c5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import uproot\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6121b6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X300_Y200/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X300_Y300/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X300_Y400/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X300_Y500/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X400_Y300/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X400_Y400/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X400_Y500/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X500_Y400/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X500_Y500/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X550_Y500/nominal/NOTAG_merged.parquet does not exist.\n",
      "Warning: File ../../../output_parquet/final_production_Syst/merged/NMSSM_X600_Y500/nominal/NOTAG_merged.parquet does not exist.\n"
     ]
    }
   ],
   "source": [
    "# Taking mass X and corresponding Y mass points\n",
    "# mass_points = [300, 400, 500, 550, 600, 650, 700, 900, 1000, 1200, 1400, 1600, 1800, 2000, 2500, 3000, 3500, 4000]  # Example mass points\n",
    "# y_values = [ 60, 70, 80, 90, 95, 100, 125, 150, 200, 300, 400, 500, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000, 2600, 3000, 3500]  # Example Y values\n",
    "\n",
    "mass_points = [300, 400, 500, 550, 600, 650, 700, 900, 1000]  # Example mass points\n",
    "y_values = [ 100, 125, 150, 200, 300, 400, 500]  # Example Y values\n",
    "\n",
    "\n",
    "\n",
    "# Load signal data from Parquet files\n",
    "signal_data = []\n",
    "for mass in mass_points:\n",
    "    for y in y_values:\n",
    "        file_path = f\"../../../output_parquet/final_production_Syst/merged/NMSSM_X{mass}_Y{y}/nominal/NOTAG_merged.parquet\"\n",
    "        \n",
    "        if os.path.exists(file_path):  # Check if file exists\n",
    "            try:\n",
    "                df = pd.read_parquet(file_path)  # Load the Parquet file\n",
    "                df[\"mass\"] = mass  \n",
    "                df[\"y_value\"] = y  # Store Y value if needed\n",
    "                df[\"label\"] = 1  # Assuming signal label\n",
    "                signal_data.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not read {file_path}. Error: {e}\")\n",
    "        else:\n",
    "            print(f\"Warning: File {file_path} does not exist.\")\n",
    "\n",
    "# Combine all signal data into a single DataFrame\n",
    "signal_df = pd.concat(signal_data, ignore_index=True) if signal_data else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deaf0df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(346281, 853)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9991d47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load background data from ROOT files\n",
    "background_files = [\n",
    "    (\"../../outputfiles/hhbbgg_analyzer-v2-trees.root\", \"/GGJets/preselection\"),\n",
    "    (\"../../outputfiles/hhbbgg_analyzer-v2-trees.root\", \"/GJetPt20To40/preselection\"),\n",
    "    (\"../../outputfiles/hhbbgg_analyzer-v2-trees.root\", \"/GJetPt40/preselection\"),\n",
    "    (\"../../outputfiles/hhbbgg_analyzer-v2-trees.root\", \"/ttHToGG/preselection\"),\n",
    "#     (\"../../outputfiles/hhbbgg_analyzer-v2-trees.root\", \"/GJetPt20To40/preselection\"),\n",
    "#     (\"../../outputfiles/hhbbgg_analyzer-v2-trees.root\", \"/GJetPt40/preselection\"),\n",
    "]\n",
    "background_data = []\n",
    "for file_path, tree_name in background_files:\n",
    "    try:\n",
    "        with uproot.open(file_path) as file:\n",
    "            tree = file[tree_name]\n",
    "            df = tree.arrays(library=\"pd\")\n",
    "            df[\"mass\"] = np.random.choice(mass_points, len(df))  # Random mass assignment\n",
    "            df[\"label\"] = 0\n",
    "            background_data.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not read {file_path}. Error: {e}\")\n",
    "\n",
    "df_background = pd.concat(background_data, ignore_index=True) if background_data else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20bc1f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and labels\n",
    "features = [\n",
    "    'bbgg_eta', 'bbgg_phi', 'lead_pho_phi', 'sublead_pho_eta', \n",
    "    'sublead_pho_phi', 'diphoton_eta', 'diphoton_phi', 'dibjet_eta', 'dibjet_phi', \n",
    "    'lead_bjet_pt', 'sublead_bjet_pt', 'lead_bjet_eta', 'lead_bjet_phi', 'sublead_bjet_eta', \n",
    "    'sublead_bjet_phi', 'sublead_bjet_PNetB', 'lead_bjet_PNetB', 'CosThetaStar_gg', \n",
    "    'CosThetaStar_jj', 'CosThetaStar_CS', 'DeltaR_jg_min', 'pholead_PtOverM', \n",
    "    'phosublead_PtOverM', 'lead_pho_mvaID', 'sublead_pho_mvaID'\n",
    "]\n",
    "# features.extend([\"mass\", \"y_value\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1d1dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random mass + y_value assignment for backgrounds (ensure this was done earlier!)\n",
    "df_background[\"mass\"] = np.random.choice(mass_points, len(df_background))\n",
    "df_background[\"y_value\"] = np.random.choice(y_values, len(df_background))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a151916f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reduce background dataset size by random sampling\n",
    "background_fraction = 1 #  20% of the background\n",
    "df_background = df_background.sample(frac=background_fraction, random_state=42)\n",
    "\n",
    "# Combine signal and background\n",
    "df_combined = pd.concat([signal_df, df_background], ignore_index=True)\n",
    "\n",
    "# Ensure df_combined is not empty\n",
    "if df_combined.empty:\n",
    "    raise ValueError(\"Error: Combined DataFrame is empty. Check input files.\")\n",
    "\n",
    "# Convert feature data to DataFrame to prevent AttributeError\n",
    "df_features = df_combined[features]\n",
    "\n",
    "# Fill missing values with column mean\n",
    "df_features = df_features.fillna(df_features.mean())\n",
    "\n",
    "# Extract features (X) and labels (y)\n",
    "X = df_features.values\n",
    "y = df_combined[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edd6c568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3591495, 83)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_background.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "785bfc25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3937776, 25)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "491ce4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into 80% train, 20% test (stratified to maintain label distribution)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Standardize features (Fit only on train, transform both)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)  \n",
    "X_test = scaler.transform(X_test)  \n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create Dataloader for training\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Optional: Create test dataloader if you want batch evaluation\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9325c9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "class ParameterizedDNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ParameterizedDNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(16),  # Helps stabilize training\n",
    "            nn.Dropout(0.2),  # Reduce dropout\n",
    "\n",
    "            nn.Linear(16, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.Dropout(0.1),  # Lower dropout to retain information\n",
    "            \n",
    "            nn.Linear(8, 1)  # Output layer (raw logits)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28afaea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "input_dim = X.shape[1]\n",
    "model = ParameterizedDNN(input_dim)\n",
    "criterion = nn.BCEWithLogitsLoss()  # Expecting raw logits\n",
    "# criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([weight]))\n",
    "optimizer = Adam(model.parameters(), lr=0.001, weight_decay=1e-5)  # Reduce learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c4e4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "train_aucs = []\n",
    "fpr_all, tpr_all, thresholds_all = [], [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    y_true_train, y_pred_train = [], []\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        X_batch, y_batch = batch\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch).squeeze()\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        y_true_train.extend(y_batch.cpu().numpy())\n",
    "        y_pred_train.extend(torch.sigmoid(outputs).detach().cpu().numpy())\n",
    "\n",
    "    # Convert predictions to binary\n",
    "    y_pred_train_binary = [1 if p > 0.5 else 0 for p in y_pred_train]\n",
    "    train_accuracy = accuracy_score(y_true_train, y_pred_train_binary)\n",
    "    train_auc = roc_auc_score(y_true_train, y_pred_train)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs_test = model(X_test_tensor.to(device)).squeeze()\n",
    "        y_pred_test = torch.sigmoid(outputs_test).cpu().numpy()\n",
    "        test_loss = criterion(outputs_test, y_test_tensor.to(device)).item()\n",
    "        \n",
    "        y_pred_test_binary = [1 if p > 0.5 else 0 for p in y_pred_test]\n",
    "        test_accuracy = accuracy_score(y_test, y_pred_test_binary)\n",
    "        test_auc = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "    # Print Progress\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "    print(f\"Train Loss: {epoch_loss / len(train_dataloader):.4f} | Train Acc: {train_accuracy:.4f} | Train AUC: {train_auc:.4f}\")\n",
    "    print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_accuracy:.4f} | Test AUC: {test_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c4f297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Evaluate model outputs on the full dataset\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_train_scores = torch.sigmoid(model(X_train_tensor.to(device))).cpu().numpy().flatten()\n",
    "    y_pred_test_scores = torch.sigmoid(model(X_test_tensor.to(device))).cpu().numpy().flatten()\n",
    "\n",
    "# Plot histograms\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Training scores\n",
    "plt.hist(y_pred_train_scores[y_train == 1], bins=50, alpha=0.6, label='Signal (Train)', color='red', histtype='stepfilled', density=True)\n",
    "plt.hist(y_pred_train_scores[y_train == 0], bins=50, alpha=0.6, label='Background (Train)', color='blue', histtype='stepfilled', density=True)\n",
    "\n",
    "# Test scores: Compute histograms, then plot centers\n",
    "def plot_points(y_scores, label, color):\n",
    "    counts, bins = np.histogram(y_scores, bins=50, range=(0, 1), density=True)\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "    plt.plot(bin_centers, counts, f'{color}o', label=label)\n",
    "\n",
    "plot_points(y_pred_test_scores[y_test == 1], 'Signal (Test)', 'r')\n",
    "plot_points(y_pred_test_scores[y_test == 0], 'Background (Test)', 'k')\n",
    "\n",
    "# Aesthetics\n",
    "plt.axvline(0.5, color='gray', linestyle='--')\n",
    "plt.xlabel(\"Classifier Output\")\n",
    "plt.ylabel(\"Normalized Events\")\n",
    "plt.title(\"Classifier Output with PyTorch\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac61c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(range(1, num_epochs+1), train_losses, marker='o', linestyle='-', color='blue')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss vs. Epochs\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65ec4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Accuracy\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(range(1, num_epochs+1), train_accuracies, marker='o', linestyle='-', color='green')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs. Epochs\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5276f5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot AUC\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(range(1, num_epochs+1), train_aucs, marker='o', linestyle='-', color='red')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.title(\"AUC vs. Epochs\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07f43a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot AUC scores over epochs\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs+1), train_aucs, label=\"AUC\", color='blue', marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUC')\n",
    "plt.title('AUC Score over Epochs')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc576ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Plot the final ROC curve\n",
    "# Select the ROC curve from the last epoch\n",
    "fpr_last = fpr_all[-1]\n",
    "tpr_last = tpr_all[-1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr_last, tpr_last, color='darkorange', lw=2, label=f'ROC curve (AUC = {train_aucs[-1]:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # Random classifier line\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title(f'Final ROC Curve (AUC = {train_aucs[-1]:.2f})')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff253e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186572ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_check = pd.DataFrame(X_train, columns=features)\n",
    "df_check['label'] = y_train\n",
    "print(df_check.corr()['label'].sort_values(ascending=False).head(10))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
